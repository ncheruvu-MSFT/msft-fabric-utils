{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4934091",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Microsoft Fabric â€“ Capacity Migration Inventory\n",
    "\n",
    "This notebook **auto-discovers** every Fabric capacity, workspace, and item in\n",
    "your tenant to produce a **migration-ready inventory** for cross-region\n",
    "capacity moves (e.g., West US â†’ East US 2).\n",
    "\n",
    "| Metric | Source |\n",
    "|--------|--------|\n",
    "| Capacity Name, Region, SKU, State | `GET /v1/capacities` |\n",
    "| Workspace Name, Capacity assignment | `GET /v1/admin/workspaces` |\n",
    "| Item Name, Type, Last Updated | `GET /v1/admin/items` |\n",
    "| Movable vs Non-Movable classification | [MS Learn â€“ Capacity reassignment restrictions](https://learn.microsoft.com/en-us/fabric/admin/portal-workspace-capacity-reassignment) |\n",
    "| Migration Complexity | Calculated rule-based |\n",
    "| Suggested Wave | Auto-assigned by complexity |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"ğŸ” Auth (auto)\"] --> B[\"ğŸ“¡ FabricRestClient\"]\n",
    "    B --> C[\"ğŸ¢ List Capacities\"]\n",
    "    B --> D[\"ğŸ“‚ List Workspaces (Admin)\"]\n",
    "    B --> E[\"ğŸ“¦ List Items (Admin)\"]\n",
    "    C --> F[\"ğŸ“Š Inventory DataFrame\"]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G[\"âœ… Movable / âŒ Non-Movable\"]\n",
    "    G --> H[\"ğŸ“ˆ Migration Complexity\"]\n",
    "    H --> I[\"ğŸ’¾ Export CSV\"]\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- This notebook runs **inside Microsoft Fabric** (not externally)\n",
    "- **Fabric Administrator** role (required for Admin APIs)\n",
    "- **No external dependencies** â€“ uses only Fabric built-in libraries:\n",
    "  - `sempy.fabric.FabricRestClient` â€“ REST API calls with automatic auth\n",
    "  - `pandas` â€“ pre-installed in Fabric runtimes\n",
    "\n",
    "## Key Microsoft Learn References\n",
    "| Topic | Link |\n",
    "|-------|------|\n",
    "| Capacity reassignment restrictions | https://learn.microsoft.com/en-us/fabric/admin/portal-workspace-capacity-reassignment |\n",
    "| Multi-Geo support | https://learn.microsoft.com/en-us/fabric/admin/service-admin-premium-multi-geo |\n",
    "| Move Power BI tenant to different region | https://learn.microsoft.com/en-us/power-bi/admin/service-admin-region-move |\n",
    "| Find Fabric home region | https://learn.microsoft.com/en-us/fabric/admin/find-fabric-home-region |\n",
    "| Admin Items API | https://learn.microsoft.com/en-us/rest/api/fabric/admin/items/list-items |\n",
    "| Admin Workspaces API | https://learn.microsoft.com/en-us/rest/api/fabric/admin/workspaces/list-workspaces |\n",
    "| Capacities API | https://learn.microsoft.com/en-us/rest/api/fabric/core/capacities/list-capacities |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809f7fe",
   "metadata": {},
   "source": [
    "## 0ï¸âƒ£ Verify Fabric Environment & Retry Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify Fabric Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Zero external dependencies â€“ only Fabric built-in libs + Python stdlib\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Initialize Fabric REST client (handles auth automatically via notebook identity)\n",
    "rest_client = fabric.FabricRestClient()\n",
    "\n",
    "print(\"âœ… Fabric environment verified (zero external dependencies)\")\n",
    "print(f\"   pandas version: {pd.__version__}\")\n",
    "print(f\"   REST client:    sempy.fabric.FabricRestClient (auto-auth)\")\n",
    "\n",
    "\n",
    "# â”€â”€ Resilient API Helper with Retry & Exponential Backoff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def fabric_api_get(client, path, max_retries=5, base_delay=1.0,\n",
    "                   max_delay=60.0):\n",
    "    \"\"\"\n",
    "    GET request via FabricRestClient with automatic retry + exponential\n",
    "    backoff for HTTP 429 (rate-limit) and transient 5xx errors.\n",
    "    Honors the Retry-After header when present.\n",
    "\n",
    "    Uses only Fabric built-in libs â€“ no 'requests' import needed.\n",
    "    \"\"\"\n",
    "    retryable = {429, 500, 502, 503, 504}\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            resp = client.get(path)\n",
    "\n",
    "            if resp.status_code not in retryable:\n",
    "                return resp\n",
    "\n",
    "            # Determine wait time\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = resp.headers.get(\"Retry-After\")\n",
    "                wait = float(retry_after) if retry_after else base_delay * (2 ** attempt)\n",
    "            else:\n",
    "                wait = base_delay * (2 ** attempt)\n",
    "\n",
    "            wait = min(wait * (0.75 + random.random() * 0.5), max_delay)  # jitter + cap\n",
    "\n",
    "            if attempt < max_retries:\n",
    "                print(f\"   â³ HTTP {resp.status_code} â€“ retrying in {wait:.1f}s \"\n",
    "                      f\"(attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                resp.raise_for_status()\n",
    "\n",
    "        except (ConnectionError, OSError):\n",
    "            if attempt < max_retries:\n",
    "                wait = min(base_delay * (2 ** attempt), max_delay)\n",
    "                print(f\"   â³ Connection error â€“ retrying in {wait:.1f}s \"\n",
    "                      f\"(attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    return resp\n",
    "\n",
    "\n",
    "print(\"âœ… Retry helper loaded (max 5 retries, exponential backoff with jitter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e3575",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Configuration\n",
    "\n",
    "No workspace-specific config needed â€“ this notebook scans the **entire tenant**\n",
    "using Fabric Admin APIs.  Optionally set a target region filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaee53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Filter: only inventorise capacities in this source region (set to None for all)\n",
    "SOURCE_REGION_FILTER = None   # e.g. \"West US\", \"East US 2\", or None for all\n",
    "\n",
    "# Target region (used for labelling in migration plan)\n",
    "TARGET_REGION = \"East US 2\"\n",
    "\n",
    "# Auth is handled automatically by FabricRestClient (initialized above)\n",
    "# No manual tokens, headers, or API base URLs needed.\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(f\"ğŸ“¦ Fabric Capacity Migration Inventory\")\n",
    "print(f\"   Source Region Filter: {SOURCE_REGION_FILTER or 'ALL'}\")\n",
    "print(f\"   Target Region:       {TARGET_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf63f8",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Discover All Capacities\n",
    "\n",
    "Uses `GET /v1/capacities` to list every capacity the caller can access.\n",
    "Returns capacity ID, display name, SKU, region, and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ List Capacities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def list_all_capacities(client):\n",
    "    \"\"\"List all Fabric capacities via FabricRestClient with pagination.\"\"\"\n",
    "    path = \"/v1/capacities\"\n",
    "    capacities = []\n",
    "    while path:\n",
    "        resp = fabric_api_get(client, path)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        capacities.extend(data.get(\"value\", []))\n",
    "        cont = data.get(\"continuationToken\")\n",
    "        path = f\"/v1/capacities?continuationToken={cont}\" if cont else None\n",
    "    return capacities\n",
    "\n",
    "\n",
    "raw_capacities = list_all_capacities(rest_client)\n",
    "\n",
    "df_capacities = pd.DataFrame([\n",
    "    {\n",
    "        \"capacity_id\":   c[\"id\"],\n",
    "        \"capacity_name\": c[\"displayName\"],\n",
    "        \"sku\":           c[\"sku\"],\n",
    "        \"region\":        c[\"region\"],\n",
    "        \"state\":         c[\"state\"],\n",
    "    }\n",
    "    for c in raw_capacities\n",
    "])\n",
    "\n",
    "# Apply region filter if specified\n",
    "if SOURCE_REGION_FILTER:\n",
    "    df_capacities = df_capacities[\n",
    "        df_capacities[\"region\"].str.lower() == SOURCE_REGION_FILTER.lower()\n",
    "    ].copy()\n",
    "\n",
    "print(f\"ğŸ¢ Found {len(df_capacities)} capacity/ies\")\n",
    "print()\n",
    "print(df_capacities[[\"capacity_name\", \"sku\", \"region\", \"state\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971d55d",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Discover All Workspaces\n",
    "\n",
    "Uses the **Admin API** `GET /v1/admin/workspaces` to list every workspace,\n",
    "then joins to capacities to get region info.\n",
    "\n",
    "> **Rate limit:** Admin API allows max 200 requests / hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ List All Workspaces (Admin API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def list_admin_workspaces(client, capacity_id=None, state=\"Active\"):\n",
    "    \"\"\"List workspaces via Admin API with pagination (uses FabricRestClient).\"\"\"\n",
    "    path = f\"/v1/admin/workspaces?state={state}\"\n",
    "    if capacity_id:\n",
    "        path += f\"&capacityId={capacity_id}\"\n",
    "\n",
    "    workspaces = []\n",
    "    while path:\n",
    "        resp = fabric_api_get(client, path)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        workspaces.extend(data.get(\"workspaces\", []))\n",
    "        cont = data.get(\"continuationToken\")\n",
    "        if cont:\n",
    "            base = path.split(\"&continuationToken\")[0].split(\"?continuationToken\")[0]\n",
    "            sep = \"&\" if \"?\" in base else \"?\"\n",
    "            path = f\"{base}{sep}continuationToken={cont}\"\n",
    "        else:\n",
    "            path = None\n",
    "        time.sleep(0.5)  # be gentle on rate limits\n",
    "    return workspaces\n",
    "\n",
    "\n",
    "# Fetch workspaces scoped to our capacity list\n",
    "all_workspaces = []\n",
    "target_capacity_ids = set(df_capacities[\"capacity_id\"])\n",
    "\n",
    "for _, cap_row in df_capacities.iterrows():\n",
    "    print(f\"   ğŸ“‚ Scanning capacity: {cap_row['capacity_name']} ({cap_row['region']}) ...\")\n",
    "    ws_list = list_admin_workspaces(rest_client, capacity_id=cap_row[\"capacity_id\"])\n",
    "    for ws in ws_list:\n",
    "        all_workspaces.append({\n",
    "            \"workspace_id\":   ws[\"id\"],\n",
    "            \"workspace_name\": ws[\"name\"],\n",
    "            \"workspace_type\": ws.get(\"type\", \"Workspace\"),\n",
    "            \"workspace_state\": ws.get(\"state\", \"Active\"),\n",
    "            \"capacity_id\":    ws.get(\"capacityId\", \"\"),\n",
    "        })\n",
    "    time.sleep(1.0)\n",
    "\n",
    "df_workspaces = pd.DataFrame(all_workspaces)\n",
    "\n",
    "# Join capacity details\n",
    "if not df_workspaces.empty:\n",
    "    df_workspaces = df_workspaces.merge(\n",
    "        df_capacities[[\"capacity_id\", \"capacity_name\", \"sku\", \"region\"]],\n",
    "        on=\"capacity_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ… Found {len(df_workspaces)} workspace(s) across {len(df_capacities)} capacity/ies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872dc57",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Discover All Items per Workspace\n",
    "\n",
    "Uses the **Admin Items API** `GET /v1/admin/items` to enumerate every item,\n",
    "then classifies each as **Movable** or **Non-Movable** per the\n",
    "[capacity reassignment restrictions](https://learn.microsoft.com/en-us/fabric/admin/portal-workspace-capacity-reassignment).\n",
    "\n",
    "### Cross-Region Movable Item Types\n",
    "| Movable âœ… | Non-Movable âŒ (must recreate) |\n",
    "|-----------|-------------------------------|\n",
    "| Report | Lakehouse |\n",
    "| Semantic model (small) | Warehouse |\n",
    "| Dashboard | Notebook |\n",
    "| Dataflow Gen1 | Eventhouse |\n",
    "| Paginated Report | KQLDatabase |\n",
    "| Datamart | DataPipeline |\n",
    "| Scorecard | SparkJobDefinition |\n",
    "| | Eventstream |\n",
    "| | MLExperiment / MLModel |\n",
    "| | Environment |\n",
    "| | SQLEndpoint |\n",
    "| | All other Fabric-native items |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ List All Items (Admin API) & Classify Movability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Per MS Learn: cross-region movable types\n",
    "MOVABLE_TYPES = {\n",
    "    \"Report\", \"SemanticModel\", \"Dashboard\",\n",
    "    \"DataflowGen1\", \"Dataflow\",\n",
    "    \"PaginatedReport\", \"Datamart\", \"Scorecard\",\n",
    "}\n",
    "\n",
    "\n",
    "def list_admin_items(client, workspace_id=None, capacity_id=None):\n",
    "    \"\"\"List items via Admin API with pagination (uses FabricRestClient).\"\"\"\n",
    "    path = \"/v1/admin/items\"\n",
    "    params_parts = []\n",
    "    if workspace_id:\n",
    "        params_parts.append(f\"workspaceId={workspace_id}\")\n",
    "    if capacity_id:\n",
    "        params_parts.append(f\"capacityId={capacity_id}\")\n",
    "    if params_parts:\n",
    "        path += \"?\" + \"&\".join(params_parts)\n",
    "\n",
    "    items = []\n",
    "    while path:\n",
    "        resp = fabric_api_get(client, path)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items.extend(data.get(\"itemEntities\", []))\n",
    "        cont = data.get(\"continuationToken\")\n",
    "        if cont:\n",
    "            base = path.split(\"&continuationToken\")[0].split(\"?continuationToken\")[0]\n",
    "            sep = \"&\" if \"?\" in base else \"?\"\n",
    "            path = f\"{base}{sep}continuationToken={cont}\"\n",
    "        else:\n",
    "            path = None\n",
    "        time.sleep(0.5)\n",
    "    return items\n",
    "\n",
    "\n",
    "# Fetch items for each capacity in scope\n",
    "all_items = []\n",
    "for _, cap_row in df_capacities.iterrows():\n",
    "    print(f\"   ğŸ“¦ Scanning items in capacity: {cap_row['capacity_name']} ...\")\n",
    "    raw_items = list_admin_items(rest_client, capacity_id=cap_row[\"capacity_id\"])\n",
    "    for item in raw_items:\n",
    "        itype = item.get(\"type\", \"Unknown\")\n",
    "        all_items.append({\n",
    "            \"item_id\":        item[\"id\"],\n",
    "            \"item_name\":      item.get(\"name\", \"\"),\n",
    "            \"item_type\":      itype,\n",
    "            \"workspace_id\":   item.get(\"workspaceId\", \"\"),\n",
    "            \"capacity_id\":    item.get(\"capacityId\", \"\"),\n",
    "            \"last_updated\":   item.get(\"lastUpdatedDate\", \"\"),\n",
    "            \"is_movable\":     itype in MOVABLE_TYPES,\n",
    "            \"movability\":    \"âœ… Movable\" if itype in MOVABLE_TYPES else \"âŒ Non-Movable\",\n",
    "        })\n",
    "    print(f\"      {len(raw_items)} item(s) found\")\n",
    "    time.sleep(1.0)\n",
    "\n",
    "df_items = pd.DataFrame(all_items)\n",
    "\n",
    "# Join workspace + capacity details\n",
    "if not df_items.empty and not df_workspaces.empty:\n",
    "    df_items = df_items.merge(\n",
    "        df_workspaces[[\"workspace_id\", \"workspace_name\", \"capacity_name\", \"sku\", \"region\"]],\n",
    "        on=\"workspace_id\", how=\"left\", suffixes=(\"\", \"_ws\")\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ… Total items discovered: {len(df_items)}\")\n",
    "if not df_items.empty:\n",
    "    movable     = df_items[df_items[\"is_movable\"] == True]\n",
    "    non_movable = df_items[df_items[\"is_movable\"] == False]\n",
    "    print(f\"   âœ… Movable:     {len(movable)}\")\n",
    "    print(f\"   âŒ Non-Movable: {len(non_movable)}\")\n",
    "    print(f\"\\nğŸ“Š Item Type Distribution:\")\n",
    "    print(df_items.groupby([\"item_type\", \"movability\"]).size()\n",
    "          .reset_index(name=\"count\")\n",
    "          .sort_values([\"movability\", \"count\"], ascending=[True, False])\n",
    "          .to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0872c73",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Calculate Migration Complexity per Workspace\n",
    "\n",
    "Assigns a **Low / Medium / High** migration complexity to each workspace\n",
    "based on the presence of non-movable items and their types.\n",
    "\n",
    "| Complexity | Criteria |\n",
    "|------------|----------|\n",
    "| **Low** | Only movable items (Reports, Semantic models, Dashboards, etc.) |\n",
    "| **Medium** | Has Lakehouses or Warehouses (data copy required) |\n",
    "| **High** | Has Notebooks, Pipelines, Eventhouses, or other complex Fabric items |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Migration Complexity & Wave Assignment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "HIGH_COMPLEXITY_TYPES = {\n",
    "    \"Notebook\", \"DataPipeline\", \"Eventhouse\", \"KQLDatabase\",\n",
    "    \"SparkJobDefinition\", \"Eventstream\", \"MLExperiment\", \"MLModel\",\n",
    "    \"Environment\", \"KQLQueryset\", \"KQLDashboard\", \"GraphQLApi\",\n",
    "    \"MirroredDatabase\", \"MirroredWarehouse\", \"Reflex\",\n",
    "    \"DataflowGen2\", \"CopyJob\", \"ApacheAirflowJob\",\n",
    "}\n",
    "\n",
    "MEDIUM_COMPLEXITY_TYPES = {\n",
    "    \"Lakehouse\", \"Warehouse\", \"SQLEndpoint\", \"SQLDatabase\",\n",
    "}\n",
    "\n",
    "\n",
    "def assess_workspace_complexity(ws_items_df):\n",
    "    \"\"\"Determine migration complexity for a workspace based on its items.\"\"\"\n",
    "    types_present = set(ws_items_df[\"item_type\"])\n",
    "    non_movable_count = ws_items_df[~ws_items_df[\"is_movable\"]].shape[0]\n",
    "\n",
    "    if types_present & HIGH_COMPLEXITY_TYPES:\n",
    "        complexity = \"High\"\n",
    "    elif types_present & MEDIUM_COMPLEXITY_TYPES:\n",
    "        complexity = \"Medium\"\n",
    "    elif non_movable_count > 0:\n",
    "        complexity = \"Medium\"\n",
    "    else:\n",
    "        complexity = \"Low\"\n",
    "\n",
    "    return complexity\n",
    "\n",
    "\n",
    "def assign_wave(complexity):\n",
    "    \"\"\"Auto-assign migration wave based on complexity.\"\"\"\n",
    "    return {\"Low\": 1, \"Medium\": 2, \"High\": 3}.get(complexity, 3)\n",
    "\n",
    "\n",
    "# Build workspace-level summary\n",
    "ws_summaries = []\n",
    "\n",
    "if not df_items.empty:\n",
    "    for ws_id, ws_group in df_items.groupby(\"workspace_id\"):\n",
    "        ws_name = ws_group[\"workspace_name\"].iloc[0] if \"workspace_name\" in ws_group.columns else \"Unknown\"\n",
    "        cap_name = ws_group[\"capacity_name_ws\"].iloc[0] if \"capacity_name_ws\" in ws_group.columns else (\n",
    "            ws_group[\"capacity_name\"].iloc[0] if \"capacity_name\" in ws_group.columns else \"Unknown\"\n",
    "        )\n",
    "        cap_sku = ws_group[\"sku\"].iloc[0] if \"sku\" in ws_group.columns else \"Unknown\"\n",
    "        cap_region = ws_group[\"region\"].iloc[0] if \"region\" in ws_group.columns else \"Unknown\"\n",
    "\n",
    "        total_items = len(ws_group)\n",
    "        movable_count = ws_group[\"is_movable\"].sum()\n",
    "        non_movable_count = total_items - movable_count\n",
    "\n",
    "        types_present = ws_group[\"item_type\"].value_counts().to_dict()\n",
    "        non_movable_types = ws_group[~ws_group[\"is_movable\"]][\"item_type\"].unique().tolist()\n",
    "\n",
    "        complexity = assess_workspace_complexity(ws_group)\n",
    "        wave = assign_wave(complexity)\n",
    "\n",
    "        ws_summaries.append({\n",
    "            \"workspace_id\":        ws_id,\n",
    "            \"workspace_name\":      ws_name,\n",
    "            \"capacity_name\":       cap_name,\n",
    "            \"sku\":                 cap_sku,\n",
    "            \"source_region\":       cap_region,\n",
    "            \"target_region\":       TARGET_REGION,\n",
    "            \"total_items\":         total_items,\n",
    "            \"movable_items\":       int(movable_count),\n",
    "            \"non_movable_items\":   int(non_movable_count),\n",
    "            \"item_types_summary\":  str(types_present),\n",
    "            \"non_movable_types\":   \", \".join(non_movable_types) if non_movable_types else \"\",\n",
    "            \"migration_complexity\": complexity,\n",
    "            \"suggested_wave\":      wave,\n",
    "            \"business_owner\":      \"\",      # â† fill manually\n",
    "            \"data_size_estimate\":  \"\",      # â† fill manually\n",
    "            \"external_deps\":      \"\",      # â† fill manually (e.g. Databricks, Gateways)\n",
    "            \"notes\":              \"\",\n",
    "        })\n",
    "\n",
    "df_ws_summary = pd.DataFrame(ws_summaries)\n",
    "\n",
    "if not df_ws_summary.empty:\n",
    "    df_ws_summary = df_ws_summary.sort_values([\"suggested_wave\", \"migration_complexity\"])\n",
    "\n",
    "print(f\"âœ… Workspace migration assessment complete: {len(df_ws_summary)} workspace(s)\")\n",
    "print(f\"   Wave 1 (Low):    {len(df_ws_summary[df_ws_summary['suggested_wave'] == 1])}\")\n",
    "print(f\"   Wave 2 (Medium): {len(df_ws_summary[df_ws_summary['suggested_wave'] == 2])}\")\n",
    "print(f\"   Wave 3 (High):   {len(df_ws_summary[df_ws_summary['suggested_wave'] == 3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e1c3e",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Migration Readiness Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Migration Readiness Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_ws_summary.empty:\n",
    "    print(\"ğŸ“¦ FABRIC CAPACITY MIGRATION â€“ READINESS REPORT\")\n",
    "    print(\"=\" * 130)\n",
    "    print(f\"   Source Region:   {SOURCE_REGION_FILTER or 'ALL'}\")\n",
    "    print(f\"   Target Region:   {TARGET_REGION}\")\n",
    "    print(f\"   Capacities:      {len(df_capacities)}\")\n",
    "    print(f\"   Workspaces:      {len(df_ws_summary)}\")\n",
    "    print(f\"   Total Items:     {df_ws_summary['total_items'].sum():,}\")\n",
    "    print(f\"   Movable Items:   {df_ws_summary['movable_items'].sum():,}\")\n",
    "    print(f\"   Non-Movable:     {df_ws_summary['non_movable_items'].sum():,}\")\n",
    "    print(\"=\" * 130)\n",
    "\n",
    "    # Per-wave breakdown\n",
    "    for wave in [1, 2, 3]:\n",
    "        wave_ws = df_ws_summary[df_ws_summary[\"suggested_wave\"] == wave]\n",
    "        if wave_ws.empty:\n",
    "            continue\n",
    "        label = {1: \"Low (Movable-only)\", 2: \"Medium (Data copy)\", 3: \"High (Recreate artifacts)\"}[wave]\n",
    "        print(f\"\\nğŸŸ¢ WAVE {wave}: {label} â€“ {len(wave_ws)} workspace(s)\")\n",
    "        print(\"-\" * 130)\n",
    "        for _, row in wave_ws.iterrows():\n",
    "            print(f\"   ğŸ“‚ {row['workspace_name']}\")\n",
    "            print(f\"      Capacity: {row['capacity_name']} ({row['sku']}) | \"\n",
    "                  f\"Region: {row['source_region']} â†’ {row['target_region']}\")\n",
    "            print(f\"      Items: {row['total_items']} total | \"\n",
    "                  f\"âœ… {row['movable_items']} movable | \"\n",
    "                  f\"âŒ {row['non_movable_items']} non-movable\")\n",
    "            if row['non_movable_types']:\n",
    "                print(f\"      Non-movable types: {row['non_movable_types']}\")\n",
    "            print()\n",
    "\n",
    "    # Constraints reminder\n",
    "    print(\"\\nâš ï¸  KEY CONSTRAINTS (from MS Learn)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"   1. Workspace reassignment CANCELS all running jobs\")\n",
    "    print(\"   2. Non-movable items must be REMOVED before cross-region reassignment\")\n",
    "    print(\"   3. Non-movable items must be RECREATED in the target region\")\n",
    "    print(\"   4. Dataflow Gen2 staging lakehouses/warehouses block migration\")\n",
    "    print(\"      (delete all Dataflow Gen2 items first, then delete staging items)\")\n",
    "    print(\"   5. Large-storage format semantic models cannot be moved between regions\")\n",
    "    print(\"   6. Private Link networking must be temporarily disabled during migration\")\n",
    "    print(\"   7. After migration, it can take up to 1 hour before new Fabric items\")\n",
    "    print(\"      can be created in the workspace\")\n",
    "    print()\n",
    "    print(\"   ğŸ“š References:\")\n",
    "    print(\"   â€¢ https://learn.microsoft.com/en-us/fabric/admin/portal-workspace-capacity-reassignment\")\n",
    "    print(\"   â€¢ https://learn.microsoft.com/en-us/fabric/admin/service-admin-premium-multi-geo\")\n",
    "    print(\"   â€¢ https://learn.microsoft.com/en-us/power-bi/admin/service-admin-region-move\")\n",
    "else:\n",
    "    print(\"âš ï¸  No workspace data to report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214b402",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Phased Migration Plan Summary\n",
    "\n",
    "Auto-generated phased plan based on the inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd73e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Phased Migration Plan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"ğŸ“‹ PHASED MIGRATION PLAN\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "phases = [\n",
    "    (\"Phase 0\", \"Discovery & Inventory\", [\n",
    "        \"âœ… Run this notebook to auto-discover all capacities, workspaces, items\",\n",
    "        \"Export inventory CSV and share with business owners\",\n",
    "        \"Identify business owners for each workspace\",\n",
    "        \"Document external dependencies (Databricks, gateways, VNets)\",\n",
    "        \"Confirm data sizes for lakehouses and warehouses\",\n",
    "        \"Agree on migration windows with stakeholders\",\n",
    "    ]),\n",
    "    (\"Phase 1\", \"Target Capacity Setup in \" + TARGET_REGION, [\n",
    "        f\"Provision new Fabric capacity/ies in {TARGET_REGION}\",\n",
    "        \"Match SKU sizing to source capacities\",\n",
    "        \"Configure networking (VNet, private endpoints if needed)\",\n",
    "        \"Set up gateways in the target region\",\n",
    "        \"Validate admin & RBAC permissions on new capacities\",\n",
    "    ]),\n",
    "    (\"Phase 2\", \"Wave 1 â€“ Low Complexity (Movable-only workspaces)\", [\n",
    "        \"Reassign workspaces with ONLY movable items to target capacity\",\n",
    "        \"Movable types: Report, Semantic model, Dashboard, Dataflow Gen1, Paginated Report, Datamart, Scorecard\",\n",
    "        \"Validate reports and semantic models post-migration\",\n",
    "        \"Update data source connections if needed\",\n",
    "    ]),\n",
    "    (\"Phase 3\", \"Wave 2 â€“ Medium Complexity (Data copy required)\", [\n",
    "        f\"Create new Lakehouses/Warehouses in {TARGET_REGION} workspaces\",\n",
    "        \"Copy data using Fabric Copy Job, AzCopy, or notebooks\",\n",
    "        \"Remove non-movable items from source workspace\",\n",
    "        \"Reassign workspace to target capacity\",\n",
    "        \"Recreate Lakehouses/Warehouses and validate data\",\n",
    "        \"Reconnect semantic models and reports to new endpoints\",\n",
    "    ]),\n",
    "    (\"Phase 4\", \"Wave 3 â€“ High Complexity (Recreate artifacts)\", [\n",
    "        \"Backup notebook code, pipeline definitions, eventhouse configs\",\n",
    "        \"Use Git integration to version-control definitions\",\n",
    "        \"Remove all non-movable items from source workspace\",\n",
    "        \"Reassign workspace to target capacity\",\n",
    "        \"Recreate Notebooks, Pipelines, Eventhouses, Spark jobs\",\n",
    "        \"Copy lakehouse data and re-point pipeline sources\",\n",
    "        \"Run end-to-end validation of data pipelines\",\n",
    "    ]),\n",
    "    (\"Phase 5\", \"Validation, Cutover & Decommission\", [\n",
    "        \"Run parallel execution for critical workloads\",\n",
    "        \"Business sign-off on data accuracy and report functionality\",\n",
    "        \"Update IP allowlists, DNS, and Azure Private Endpoints\",\n",
    "        \"Decommission source capacities in old region\",\n",
    "        \"Archive migration documentation\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for phase_id, phase_name, steps in phases:\n",
    "    print(f\"\\nğŸ“Œ {phase_id}: {phase_name}\")\n",
    "    print(\"-\" * 90)\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        print(f\"   {i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65230780",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Export Inventory to CSV\n",
    "\n",
    "Exports three CSV files:\n",
    "1. **Capacities** â€“ all capacities with region and SKU\n",
    "2. **Workspace Summary** â€“ migration complexity, wave, movable/non-movable counts\n",
    "3. **Item Detail** â€“ every item with movability classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export Inventory to CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Capacities\n",
    "if not df_capacities.empty:\n",
    "    f1 = f\"migration_capacities_{timestamp}.csv\"\n",
    "    df_capacities.to_csv(f1, index=False)\n",
    "    print(f\"âœ… Capacities exported: {f1} ({len(df_capacities)} rows)\")\n",
    "\n",
    "# Workspace summary\n",
    "if not df_ws_summary.empty:\n",
    "    f2 = f\"migration_workspace_summary_{timestamp}.csv\"\n",
    "    df_ws_summary.to_csv(f2, index=False)\n",
    "    print(f\"âœ… Workspace summary exported: {f2} ({len(df_ws_summary)} rows)\")\n",
    "\n",
    "# Item detail\n",
    "if not df_items.empty:\n",
    "    f3 = f\"migration_item_detail_{timestamp}.csv\"\n",
    "    export_cols = [c for c in [\n",
    "        \"item_id\", \"item_name\", \"item_type\", \"movability\",\n",
    "        \"workspace_id\", \"workspace_name\", \"capacity_name\",\n",
    "        \"sku\", \"region\", \"last_updated\"\n",
    "    ] if c in df_items.columns]\n",
    "    df_items[export_cols].to_csv(f3, index=False)\n",
    "    print(f\"âœ… Item detail exported: {f3} ({len(df_items)} rows)\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Use these CSV files to populate the Excel Migration Tracker template.\")\n",
    "print(\"   See: fabric-region-migration/generate_migration_tracker_excel.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95d60d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Notes\n",
    "\n",
    "### Zero External Dependencies\n",
    "\n",
    "This notebook uses **only Fabric built-in libraries** â€“ no `pip install` required:\n",
    "\n",
    "| Library | Purpose | Pre-installed |\n",
    "|---------|---------|---------------|\n",
    "| `sempy.fabric.FabricRestClient` | REST API calls with automatic auth | âœ… Yes (semantic-link) |\n",
    "| `pandas` | DataFrames, analysis, CSV export | âœ… Yes |\n",
    "| `json`, `time`, `random`, `datetime` | Python standard library | âœ… Yes |\n",
    "\n",
    "### Fabric REST API Endpoints Used\n",
    "\n",
    "| Endpoint | Purpose | Rate Limit |\n",
    "|----------|---------|------------|\n",
    "| `GET /v1/capacities` | List all capacities | Standard |\n",
    "| `GET /v1/admin/workspaces` | List all workspaces (tenant-wide) | 200 req/hr |\n",
    "| `GET /v1/admin/items` | List all items (tenant-wide) | 200 req/hr |\n",
    "\n",
    "### Cross-Region Movable Items (MS Learn)\n",
    "\n",
    "Only these types can move between regions via workspace reassignment:\n",
    "- Report, Semantic model (small storage), Dashboard\n",
    "- Dataflow Gen1, Paginated Report, Datamart, Scorecard\n",
    "\n",
    "**All other Fabric-native items** (Lakehouse, Warehouse, Notebook, Pipeline,\n",
    "Eventhouse, etc.) must be **deleted from the source workspace**, the workspace\n",
    "reassigned, then the items **recreated in the target region**.\n",
    "\n",
    "### Permissions Required\n",
    "- **Fabric Administrator** or **Service Principal** with `Tenant.Read.All` scope\n",
    "- For capacity listing: `Capacity.Read.All`\n",
    "\n",
    "### Key Microsoft Learn References\n",
    "- [Capacity reassignment restrictions](https://learn.microsoft.com/en-us/fabric/admin/portal-workspace-capacity-reassignment)\n",
    "- [Multi-Geo support for Fabric](https://learn.microsoft.com/en-us/fabric/admin/service-admin-premium-multi-geo)\n",
    "- [Move Power BI tenant to different region](https://learn.microsoft.com/en-us/power-bi/admin/service-admin-region-move)\n",
    "- [Find your Fabric home region](https://learn.microsoft.com/en-us/fabric/admin/find-fabric-home-region)\n",
    "- [Admin Items API](https://learn.microsoft.com/en-us/rest/api/fabric/admin/items/list-items)\n",
    "- [Admin Workspaces API](https://learn.microsoft.com/en-us/rest/api/fabric/admin/workspaces/list-workspaces)\n",
    "- [Capacities API](https://learn.microsoft.com/en-us/rest/api/fabric/core/capacities/list-capacities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
