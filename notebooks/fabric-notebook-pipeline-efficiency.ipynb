{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c66582",
   "metadata": {},
   "source": [
    "# ğŸ“Š Microsoft Fabric â€“ Notebook & Pipeline Efficiency Monitor\n",
    "\n",
    "This notebook discovers all **Notebooks** and **Data Pipelines** in a Microsoft Fabric workspace and analyzes their **execution efficiency** using the Fabric REST API.\n",
    "\n",
    "| Metric | Source |\n",
    "|--------|--------|\n",
    "| Item Name & Type | Fabric REST API â€“ Items |\n",
    "| Run History (Start, End, Status) | Fabric REST API â€“ Job Instances |\n",
    "| Duration (seconds / minutes) | Calculated from job start & end times |\n",
    "| Success / Failure Rate | Calculated from job status |\n",
    "| Average / P50 / P95 Duration | Statistical analysis over run history |\n",
    "| Spark Resource Efficiency % | Fabric Spark Applications API + job instance analysis |\n",
    "| Capacity Unit (CU) Seconds | Fabric REST API â€“ Job Details (when available) |\n",
    "| Error Messages | Fabric REST API â€“ Failed job details |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"ğŸ” Fabric Auth\"] --> B[\"ğŸ“¡ Fabric REST API\"]\n",
    "    B --> C[\"ğŸ““ List Notebooks\"]\n",
    "    B --> D[\"ğŸ”„ List Pipelines\"]\n",
    "    C --> E[\"ğŸ“œ Job Run History\"]\n",
    "    D --> E\n",
    "    E --> F[\"ğŸ“Š Efficiency Metrics\"]\n",
    "    E --> S[\"âš¡ Spark Resource Efficiency\"]\n",
    "    F --> G[\"ğŸ“‹ Consolidated Report\"]\n",
    "    S --> G\n",
    "    G --> H[\"ğŸ’¾ CSV Export\"]\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- This notebook runs **inside Microsoft Fabric** (not externally)\n",
    "- Fabric workspace Admin or Member role\n",
    "- Pre-installed libraries: `sempy`, `mssparkutils`, `pandas`, `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e1b12",
   "metadata": {},
   "source": [
    "## 0ï¸âƒ£ Verify Fabric Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify Fabric Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "from notebookutils import mssparkutils\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "\n",
    "print(\"âœ… Fabric environment verified\")\n",
    "print(f\"   sempy version:      {fabric.__version__ if hasattr(fabric, '__version__') else 'OK'}\")\n",
    "print(f\"   mssparkutils:       available\")\n",
    "print(f\"   pandas version:     {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b66980",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Configuration\n",
    "\n",
    "Set your Fabric workspace ID and analysis parameters below.\n",
    "You can find your **Workspace ID** in the Fabric portal URL:\n",
    "`https://app.fabric.microsoft.com/groups/<WORKSPACE_ID>/...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ Fabric Workspace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to None to use the current workspace, or specify a workspace GUID\n",
    "TARGET_WORKSPACE_ID = None\n",
    "\n",
    "# â”€â”€ Analysis Window â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Number of days of run history to analyze (max depends on Fabric retention)\n",
    "LOOKBACK_DAYS = 30\n",
    "\n",
    "# â”€â”€ Item Filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to True/False to include/exclude specific item types\n",
    "INCLUDE_NOTEBOOKS = True\n",
    "INCLUDE_PIPELINES = True\n",
    "\n",
    "# â”€â”€ Resolve workspace ID â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if TARGET_WORKSPACE_ID:\n",
    "    WORKSPACE_ID = TARGET_WORKSPACE_ID\n",
    "else:\n",
    "    WORKSPACE_ID = fabric.get_workspace_id()\n",
    "\n",
    "# â”€â”€ Fabric API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "# â”€â”€ Authentication via mssparkutils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fabric_token = mssparkutils.credentials.getToken(\"pbi\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {fabric_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# â”€â”€ Display Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# â”€â”€ Validate â€“ fetch workspace info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "resp = requests.get(f\"{FABRIC_API_BASE}/workspaces/{WORKSPACE_ID}\", headers=HEADERS)\n",
    "resp.raise_for_status()\n",
    "ws = resp.json()\n",
    "print(f\"ğŸ“Š Notebook & Pipeline Efficiency Monitor\")\n",
    "print(f\"   Workspace:      {ws['displayName']}\")\n",
    "print(f\"   Workspace ID:   {WORKSPACE_ID}\")\n",
    "print(f\"   Capacity:       {ws.get('capacityId', 'N/A')}\")\n",
    "print(f\"   Lookback:       {LOOKBACK_DAYS} days\")\n",
    "print(f\"   Include:        {'Notebooks' if INCLUDE_NOTEBOOKS else ''} {'Pipelines' if INCLUDE_PIPELINES else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Discover Notebooks & Data Pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def list_fabric_items_by_type(workspace_id, headers, item_type=None):\n",
    "    \"\"\"\n",
    "    List items in a Fabric workspace. Optionally filter by type.\n",
    "    Handles pagination via continuationToken.\n",
    "    \"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/workspaces/{workspace_id}/items\"\n",
    "    if item_type:\n",
    "        url += f\"?type={item_type}\"\n",
    "\n",
    "    items = []\n",
    "    while url:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items.extend(data.get(\"value\", []))\n",
    "        # Handle pagination\n",
    "        continuation_token = data.get(\"continuationToken\")\n",
    "        if continuation_token:\n",
    "            separator = \"&\" if \"?\" in url.split(\"continuationToken\")[0] else \"?\"\n",
    "            base_url = url.split(\"&continuationToken\")[0].split(\"?continuationToken\")[0]\n",
    "            url = f\"{base_url}{separator}continuationToken={continuation_token}\"\n",
    "        else:\n",
    "            url = None\n",
    "    return items\n",
    "\n",
    "\n",
    "# â”€â”€ Notebooks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "notebooks = []\n",
    "if INCLUDE_NOTEBOOKS:\n",
    "    raw_notebooks = list_fabric_items_by_type(WORKSPACE_ID, HEADERS, \"Notebook\")\n",
    "    for nb in raw_notebooks:\n",
    "        notebooks.append({\n",
    "            \"item_id\":   nb[\"id\"],\n",
    "            \"item_name\": nb[\"displayName\"],\n",
    "            \"item_type\": \"Notebook\",\n",
    "            \"description\": nb.get(\"description\", \"\"),\n",
    "        })\n",
    "    print(f\"ğŸ““ Found {len(notebooks)} Notebook(s)\")\n",
    "\n",
    "# â”€â”€ Data Pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipelines = []\n",
    "if INCLUDE_PIPELINES:\n",
    "    raw_pipelines = list_fabric_items_by_type(WORKSPACE_ID, HEADERS, \"DataPipeline\")\n",
    "    for dp in raw_pipelines:\n",
    "        pipelines.append({\n",
    "            \"item_id\":   dp[\"id\"],\n",
    "            \"item_name\": dp[\"displayName\"],\n",
    "            \"item_type\": \"DataPipeline\",\n",
    "            \"description\": dp.get(\"description\", \"\"),\n",
    "        })\n",
    "    print(f\"ğŸ”„ Found {len(pipelines)} Data Pipeline(s)\")\n",
    "\n",
    "# â”€â”€ Combined list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_items = notebooks + pipelines\n",
    "df_items = pd.DataFrame(all_items)\n",
    "\n",
    "print(f\"\\nâœ… Total: {len(all_items)} item(s) to analyze\")\n",
    "for item in all_items:\n",
    "    print(f\"   {item['item_type']:14s} | {item['item_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b029dcf",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Fetch Run History (Job Instances)\n",
    "\n",
    "Queries the Fabric REST API for **job instances** (run history) for each Notebook and Pipeline.\n",
    "This captures start time, end time, status, and failure details for every execution.\n",
    "\n",
    "> **Note:** The Fabric API returns job instances based on retention policies of your capacity.\n",
    "> Adjust `LOOKBACK_DAYS` in the Configuration cell to control the analysis window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Fetch Run History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def get_job_instances(workspace_id, item_id, headers, lookback_days=30):\n",
    "    \"\"\"\n",
    "    Retrieve job instances (run history) for a specific Fabric item.\n",
    "    Uses GET /v1/workspaces/{workspaceId}/items/{itemId}/jobs/instances\n",
    "    \"\"\"\n",
    "    url = (f\"{FABRIC_API_BASE}/workspaces/{workspace_id}\"\n",
    "           f\"/items/{item_id}/jobs/instances\")\n",
    "\n",
    "    all_instances = []\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(days=lookback_days)\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers)\n",
    "            if resp.status_code == 404:\n",
    "                # Item may not support job instances\n",
    "                return []\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            instances = data.get(\"value\", [])\n",
    "            all_instances.extend(instances)\n",
    "\n",
    "            # Handle pagination\n",
    "            continuation_token = data.get(\"continuationToken\")\n",
    "            if continuation_token:\n",
    "                base_url = url.split(\"?continuationToken\")[0].split(\"&continuationToken\")[0]\n",
    "                separator = \"&\" if \"?\" in base_url else \"?\"\n",
    "                url = f\"{base_url}{separator}continuationToken={continuation_token}\"\n",
    "            else:\n",
    "                url = None\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code in (400, 403, 404):\n",
    "                return []\n",
    "            raise\n",
    "\n",
    "    return all_instances\n",
    "\n",
    "\n",
    "def parse_datetime(dt_str):\n",
    "    \"\"\"Parse ISO datetime string, handling various formats. Always returns timezone-aware (UTC).\"\"\"\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    try:\n",
    "        # Handle 'Z' suffix and microseconds\n",
    "        dt_str = dt_str.replace(\"Z\", \"+00:00\")\n",
    "        dt = datetime.fromisoformat(dt_str)\n",
    "        # Ensure timezone-aware â€“ if naive, assume UTC\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# â”€â”€ Collect run history for all items â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_runs = []\n",
    "items_with_runs = 0\n",
    "items_without_runs = 0\n",
    "\n",
    "for item in all_items:\n",
    "    item_id   = item[\"item_id\"]\n",
    "    item_name = item[\"item_name\"]\n",
    "    item_type = item[\"item_type\"]\n",
    "\n",
    "    print(f\"ğŸ“œ Fetching run history for {item_type}: {item_name} ...\")\n",
    "\n",
    "    instances = get_job_instances(WORKSPACE_ID, item_id, HEADERS, LOOKBACK_DAYS)\n",
    "\n",
    "    if instances:\n",
    "        items_with_runs += 1\n",
    "        for inst in instances:\n",
    "            start_time = parse_datetime(inst.get(\"startTimeUtc\"))\n",
    "            end_time = parse_datetime(inst.get(\"endTimeUtc\"))\n",
    "\n",
    "            # Calculate duration\n",
    "            duration_seconds = None\n",
    "            if start_time and end_time:\n",
    "                duration_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Filter by lookback window\n",
    "            cutoff = datetime.now(timezone.utc) - timedelta(days=LOOKBACK_DAYS)\n",
    "            if start_time and start_time < cutoff:\n",
    "                continue\n",
    "\n",
    "            all_runs.append({\n",
    "                \"item_id\":          item_id,\n",
    "                \"item_name\":        item_name,\n",
    "                \"item_type\":        item_type,\n",
    "                \"job_instance_id\":  inst.get(\"id\", \"\"),\n",
    "                \"job_type\":         inst.get(\"jobType\", \"Unknown\"),\n",
    "                \"invoke_type\":      inst.get(\"invokeType\", \"Unknown\"),\n",
    "                \"status\":           inst.get(\"status\", \"Unknown\"),\n",
    "                \"start_time\":       start_time,\n",
    "                \"end_time\":         end_time,\n",
    "                \"duration_seconds\": duration_seconds,\n",
    "                \"failure_reason\":   inst.get(\"failureReason\", {}).get(\"message\", \"\")\n",
    "                                    if isinstance(inst.get(\"failureReason\"), dict)\n",
    "                                    else str(inst.get(\"failureReason\", \"\")),\n",
    "                \"root_activity_id\": inst.get(\"rootActivityId\", \"\"),\n",
    "            })\n",
    "\n",
    "        print(f\"   âœ… {len([r for r in all_runs if r['item_id'] == item_id])} run(s) within lookback window\")\n",
    "    else:\n",
    "        items_without_runs += 1\n",
    "        print(f\"   â„¹ï¸  No run history found\")\n",
    "\n",
    "    # Rate limiting â€“ small delay between API calls\n",
    "    time.sleep(0.2)\n",
    "\n",
    "df_runs = pd.DataFrame(all_runs)\n",
    "\n",
    "print(f\"\\nğŸ“Š Run History Summary\")\n",
    "print(f\"   Items with runs:    {items_with_runs}\")\n",
    "print(f\"   Items without runs: {items_without_runs}\")\n",
    "print(f\"   Total runs found:   {len(all_runs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad191b",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Calculate Efficiency Metrics\n",
    "\n",
    "Computes per-item efficiency metrics from the run history:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Total Runs** | Number of executions in the lookback window |\n",
    "| **Success / Failure Count** | Runs by status |\n",
    "| **Success Rate (%)** | Percentage of successful runs |\n",
    "| **Avg Duration (min)** | Mean execution time |\n",
    "| **P50 Duration (min)** | Median execution time |\n",
    "| **P95 Duration (min)** | 95th percentile execution time |\n",
    "| **Min / Max Duration (min)** | Shortest and longest execution |\n",
    "| **Total Compute Time (hrs)** | Cumulative execution time |\n",
    "| **Last Run** | Most recent execution timestamp |\n",
    "| **Last Status** | Status of the most recent run |\n",
    "| **Trend** | Duration trend (improving / degrading / stable) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5406a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Calculate Efficiency Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_efficiency_metrics(df_runs):\n",
    "    \"\"\"\n",
    "    Calculate per-item efficiency metrics from run history.\n",
    "    Returns a DataFrame with one row per item.\n",
    "    \"\"\"\n",
    "    if df_runs.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for (item_id, item_name, item_type), group in df_runs.groupby(\n",
    "        [\"item_id\", \"item_name\", \"item_type\"]\n",
    "    ):\n",
    "        total_runs = len(group)\n",
    "        successful = group[group[\"status\"].str.lower().isin([\"completed\", \"succeeded\"])]\n",
    "        failed = group[group[\"status\"].str.lower().isin([\"failed\", \"cancelled\", \"deduped\"])]\n",
    "        in_progress = group[group[\"status\"].str.lower().isin([\"inprogress\", \"running\", \"notstarted\"])]\n",
    "\n",
    "        success_count = len(successful)\n",
    "        failure_count = len(failed)\n",
    "        in_progress_count = len(in_progress)\n",
    "        success_rate = (success_count / total_runs * 100) if total_runs > 0 else 0\n",
    "\n",
    "        # Duration stats (only for completed runs with valid duration)\n",
    "        durations = group[\"duration_seconds\"].dropna()\n",
    "        durations = durations[durations >= 0]\n",
    "\n",
    "        avg_duration_sec = durations.mean() if len(durations) > 0 else None\n",
    "        p50_duration_sec = durations.median() if len(durations) > 0 else None\n",
    "        p95_duration_sec = durations.quantile(0.95) if len(durations) > 0 else None\n",
    "        min_duration_sec = durations.min() if len(durations) > 0 else None\n",
    "        max_duration_sec = durations.max() if len(durations) > 0 else None\n",
    "        total_compute_sec = durations.sum() if len(durations) > 0 else 0\n",
    "\n",
    "        # Last run info\n",
    "        sorted_runs = group.sort_values(\"start_time\", ascending=False)\n",
    "        last_run_time = sorted_runs[\"start_time\"].iloc[0] if not sorted_runs.empty else None\n",
    "        last_status = sorted_runs[\"status\"].iloc[0] if not sorted_runs.empty else \"N/A\"\n",
    "\n",
    "        # Duration trend (compare first half vs second half of runs)\n",
    "        trend = \"N/A\"\n",
    "        if len(durations) >= 4:\n",
    "            sorted_group = group.dropna(subset=[\"start_time\", \"duration_seconds\"]).sort_values(\"start_time\")\n",
    "            if len(sorted_group) >= 4:\n",
    "                mid = len(sorted_group) // 2\n",
    "                first_half_avg = sorted_group.iloc[:mid][\"duration_seconds\"].mean()\n",
    "                second_half_avg = sorted_group.iloc[mid:][\"duration_seconds\"].mean()\n",
    "                if first_half_avg > 0:\n",
    "                    change_pct = ((second_half_avg - first_half_avg) / first_half_avg) * 100\n",
    "                    if change_pct < -10:\n",
    "                        trend = f\"â¬†ï¸ Improving ({change_pct:+.0f}%)\"\n",
    "                    elif change_pct > 10:\n",
    "                        trend = f\"â¬‡ï¸ Degrading ({change_pct:+.0f}%)\"\n",
    "                    else:\n",
    "                        trend = f\"â¡ï¸ Stable ({change_pct:+.0f}%)\"\n",
    "\n",
    "        # Most common failure reason\n",
    "        failure_reasons = group[group[\"failure_reason\"].str.len() > 0][\"failure_reason\"]\n",
    "        top_failure = failure_reasons.mode().iloc[0] if len(failure_reasons) > 0 else \"\"\n",
    "\n",
    "        # Invoke type distribution\n",
    "        invoke_dist = group[\"invoke_type\"].value_counts().to_dict()\n",
    "\n",
    "        metrics.append({\n",
    "            \"item_id\":              item_id,\n",
    "            \"item_name\":            item_name,\n",
    "            \"item_type\":            item_type,\n",
    "            \"total_runs\":           total_runs,\n",
    "            \"success_count\":        success_count,\n",
    "            \"failure_count\":        failure_count,\n",
    "            \"in_progress_count\":    in_progress_count,\n",
    "            \"success_rate_pct\":     round(success_rate, 1),\n",
    "            \"avg_duration_min\":     round(avg_duration_sec / 60, 2) if avg_duration_sec else None,\n",
    "            \"p50_duration_min\":     round(p50_duration_sec / 60, 2) if p50_duration_sec else None,\n",
    "            \"p95_duration_min\":     round(p95_duration_sec / 60, 2) if p95_duration_sec else None,\n",
    "            \"min_duration_min\":     round(min_duration_sec / 60, 2) if min_duration_sec else None,\n",
    "            \"max_duration_min\":     round(max_duration_sec / 60, 2) if max_duration_sec else None,\n",
    "            \"total_compute_hrs\":    round(total_compute_sec / 3600, 2) if total_compute_sec else 0,\n",
    "            \"last_run_time\":        last_run_time,\n",
    "            \"last_status\":          last_status,\n",
    "            \"duration_trend\":       trend,\n",
    "            \"top_failure_reason\":   top_failure[:100] if top_failure else \"\",\n",
    "            \"invoke_types\":         str(invoke_dist),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "if not df_runs.empty:\n",
    "    df_efficiency = calculate_efficiency_metrics(df_runs)\n",
    "    print(f\"âœ… Efficiency metrics calculated for {len(df_efficiency)} item(s)\")\n",
    "else:\n",
    "    df_efficiency = pd.DataFrame()\n",
    "    print(\"âš ï¸  No run data available to calculate efficiency metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdccc9",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Efficiency Report â€“ All Items\n",
    "\n",
    "Consolidated view showing efficiency metrics for every Notebook and Pipeline.\n",
    "Items are ranked by success rate and then by total runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9bdc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Efficiency Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_efficiency.empty:\n",
    "    # Sort: lowest success rate first (most problematic items on top)\n",
    "    df_sorted = df_efficiency.sort_values(\n",
    "        [\"success_rate_pct\", \"total_runs\"],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    display_cols = [\n",
    "        \"item_type\", \"item_name\", \"total_runs\",\n",
    "        \"success_count\", \"failure_count\", \"success_rate_pct\",\n",
    "        \"avg_duration_min\", \"p50_duration_min\", \"p95_duration_min\",\n",
    "        \"total_compute_hrs\", \"last_status\", \"duration_trend\"\n",
    "    ]\n",
    "\n",
    "    df_display = df_sorted[display_cols].rename(columns={\n",
    "        \"item_type\":         \"Type\",\n",
    "        \"item_name\":         \"Name\",\n",
    "        \"total_runs\":        \"Runs\",\n",
    "        \"success_count\":     \"âœ… Pass\",\n",
    "        \"failure_count\":     \"âŒ Fail\",\n",
    "        \"success_rate_pct\":  \"Success %\",\n",
    "        \"avg_duration_min\":  \"Avg (min)\",\n",
    "        \"p50_duration_min\":  \"P50 (min)\",\n",
    "        \"p95_duration_min\":  \"P95 (min)\",\n",
    "        \"total_compute_hrs\": \"Total (hrs)\",\n",
    "        \"last_status\":       \"Last Status\",\n",
    "        \"duration_trend\":    \"Trend\",\n",
    "    })\n",
    "\n",
    "    print(\"ğŸ“‹ NOTEBOOK & PIPELINE EFFICIENCY REPORT\")\n",
    "    print(\"=\" * 140)\n",
    "    print(f\"   Workspace:    {ws['displayName']}\")\n",
    "    print(f\"   Period:       Last {LOOKBACK_DAYS} days\")\n",
    "    print(f\"   Items:        {len(df_efficiency)} ({len(df_efficiency[df_efficiency['item_type']=='Notebook'])} Notebooks, \"\n",
    "          f\"{len(df_efficiency[df_efficiency['item_type']=='DataPipeline'])} Pipelines)\")\n",
    "    total_runs = df_efficiency['total_runs'].sum()\n",
    "    overall_success = (df_efficiency['success_count'].sum() / total_runs * 100) if total_runs > 0 else 0\n",
    "    print(f\"   Total Runs:   {total_runs:,}\")\n",
    "    print(f\"   Overall Success Rate: {overall_success:.1f}%\")\n",
    "    total_compute = df_efficiency['total_compute_hrs'].sum()\n",
    "    print(f\"   Total Compute Time:   {total_compute:,.1f} hours\")\n",
    "    print(\"=\" * 140)\n",
    "    print()\n",
    "    print(df_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  No efficiency data to display. Ensure previous cells ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17050c7",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Health Dashboard â€“ At-a-Glance\n",
    "\n",
    "Color-coded summary showing item health based on success rate thresholds:\n",
    "- ğŸŸ¢ **Healthy**: Success rate â‰¥ 95%\n",
    "- ğŸŸ¡ **Warning**: Success rate 80%â€“94%\n",
    "- ğŸ”´ **Critical**: Success rate < 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Health Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_efficiency.empty:\n",
    "    def health_badge(rate):\n",
    "        if rate >= 95:\n",
    "            return \"ğŸŸ¢ Healthy\"\n",
    "        elif rate >= 80:\n",
    "            return \"ğŸŸ¡ Warning\"\n",
    "        else:\n",
    "            return \"ğŸ”´ Critical\"\n",
    "\n",
    "    df_health = df_efficiency.copy()\n",
    "    df_health[\"health\"] = df_health[\"success_rate_pct\"].apply(health_badge)\n",
    "\n",
    "    print(\"ğŸ¥ HEALTH DASHBOARD\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Summary counts\n",
    "    healthy  = len(df_health[df_health[\"success_rate_pct\"] >= 95])\n",
    "    warning  = len(df_health[(df_health[\"success_rate_pct\"] >= 80) & (df_health[\"success_rate_pct\"] < 95)])\n",
    "    critical = len(df_health[df_health[\"success_rate_pct\"] < 80])\n",
    "    never_run = len(df_items) - len(df_efficiency) if not df_items.empty else 0\n",
    "\n",
    "    print(f\"   ğŸŸ¢ Healthy:    {healthy} item(s)\")\n",
    "    print(f\"   ğŸŸ¡ Warning:    {warning} item(s)\")\n",
    "    print(f\"   ğŸ”´ Critical:   {critical} item(s)\")\n",
    "    if never_run > 0:\n",
    "        print(f\"   âšª Never Run:  {never_run} item(s)\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Detail by health status\n",
    "    for status in [\"ğŸ”´ Critical\", \"ğŸŸ¡ Warning\", \"ğŸŸ¢ Healthy\"]:\n",
    "        subset = df_health[df_health[\"health\"] == status].sort_values(\"success_rate_pct\")\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        print(f\"\\n{status} ({len(subset)} items)\")\n",
    "        print(\"-\" * 100)\n",
    "        for _, row in subset.iterrows():\n",
    "            icon = \"ğŸ““\" if row[\"item_type\"] == \"Notebook\" else \"ğŸ”„\"\n",
    "            last_run_str = row[\"last_run_time\"].strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row[\"last_run_time\"]) else \"N/A\"\n",
    "            print(f\"   {icon} {row['item_name']}\")\n",
    "            print(f\"      Runs: {row['total_runs']:,}  |  Success: {row['success_rate_pct']}%  \"\n",
    "                  f\"|  Avg: {row['avg_duration_min']:.1f} min  |  Last: {last_run_str}  {row['duration_trend']}\")\n",
    "            if row[\"top_failure_reason\"]:\n",
    "                print(f\"      âš ï¸ Top Failure: {row['top_failure_reason'][:80]}\")\n",
    "        print()\n",
    "\n",
    "    # Items that have never been executed\n",
    "    if not df_items.empty:\n",
    "        executed_ids = set(df_efficiency[\"item_id\"])\n",
    "        never_executed = df_items[~df_items[\"item_id\"].isin(executed_ids)]\n",
    "        if not never_executed.empty:\n",
    "            print(f\"\\nâšª NEVER EXECUTED ({len(never_executed)} items)\")\n",
    "            print(\"-\" * 60)\n",
    "            for _, row in never_executed.iterrows():\n",
    "                icon = \"ğŸ““\" if row[\"item_type\"] == \"Notebook\" else \"ğŸ”„\"\n",
    "                print(f\"   {icon} {row['item_name']} ({row['item_type']})\")\n",
    "else:\n",
    "    print(\"âš ï¸  No data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e25b1",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Failure Analysis\n",
    "\n",
    "Deep dive into failed runs â€“ identifies the most common failure reasons and the items with the highest failure rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ffc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Failure Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_runs.empty:\n",
    "    failed_runs = df_runs[df_runs[\"status\"].str.lower().isin([\"failed\", \"cancelled\"])]\n",
    "\n",
    "    if not failed_runs.empty:\n",
    "        print(\"âŒ FAILURE ANALYSIS\")\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"   Total Failed/Cancelled Runs: {len(failed_runs)}\")\n",
    "        print()\n",
    "\n",
    "        # â”€â”€ Top failing items â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"ğŸ“Š Top Failing Items\")\n",
    "        print(\"-\" * 80)\n",
    "        fail_by_item = failed_runs.groupby([\"item_type\", \"item_name\"]).agg(\n",
    "            fail_count=(\"status\", \"count\"),\n",
    "        ).reset_index().sort_values(\"fail_count\", ascending=False).head(10)\n",
    "\n",
    "        for _, row in fail_by_item.iterrows():\n",
    "            icon = \"ğŸ““\" if row[\"item_type\"] == \"Notebook\" else \"ğŸ”„\"\n",
    "            print(f\"   {icon} {row['item_name']}: {row['fail_count']} failure(s)\")\n",
    "\n",
    "        # â”€â”€ Most common failure reasons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"\\nğŸ“‹ Most Common Failure Reasons\")\n",
    "        print(\"-\" * 80)\n",
    "        reasons = failed_runs[failed_runs[\"failure_reason\"].str.len() > 0][\"failure_reason\"]\n",
    "        if not reasons.empty:\n",
    "            top_reasons = reasons.value_counts().head(10)\n",
    "            for reason, count in top_reasons.items():\n",
    "                truncated = reason[:100] + \"...\" if len(reason) > 100 else reason\n",
    "                print(f\"   [{count}x] {truncated}\")\n",
    "        else:\n",
    "            print(\"   No failure reasons recorded in the API response.\")\n",
    "\n",
    "        # â”€â”€ Recent failures timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"\\nğŸ“… Recent Failures (Last 10)\")\n",
    "        print(\"-\" * 120)\n",
    "        recent_failures = failed_runs.dropna(subset=[\"start_time\"]).sort_values(\n",
    "            \"start_time\", ascending=False\n",
    "        ).head(10)\n",
    "\n",
    "        for _, row in recent_failures.iterrows():\n",
    "            start_str = row[\"start_time\"].strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row[\"start_time\"]) else \"N/A\"\n",
    "            dur = f\"{row['duration_seconds']/60:.1f} min\" if pd.notna(row[\"duration_seconds\"]) else \"N/A\"\n",
    "            reason = row[\"failure_reason\"][:60] + \"...\" if len(str(row[\"failure_reason\"])) > 60 else row[\"failure_reason\"]\n",
    "            icon = \"ğŸ““\" if row[\"item_type\"] == \"Notebook\" else \"ğŸ”„\"\n",
    "            print(f\"   {start_str} | {icon} {row['item_name'][:30]:30s} | {dur:>10s} | {reason}\")\n",
    "    else:\n",
    "        print(\"âœ… No failed runs in the analysis window â€“ all executions succeeded!\")\n",
    "else:\n",
    "    print(\"âš ï¸  No run data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518c8a1",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Run Duration Trends\n",
    "\n",
    "Shows how execution duration has changed over time for the most active items.\n",
    "Helps identify performance degradation or improvement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39370f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Run Duration Trends â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_runs.empty:\n",
    "    # Only analyze completed runs with valid duration\n",
    "    completed = df_runs[\n",
    "        (df_runs[\"status\"].str.lower().isin([\"completed\", \"succeeded\"])) &\n",
    "        (df_runs[\"duration_seconds\"].notna()) &\n",
    "        (df_runs[\"start_time\"].notna())\n",
    "    ].copy()\n",
    "\n",
    "    if not completed.empty:\n",
    "        completed[\"run_date\"] = completed[\"start_time\"].dt.date\n",
    "\n",
    "        # Top 10 most frequently run items\n",
    "        top_items = completed[\"item_name\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "        print(\"ğŸ“ˆ DURATION TRENDS (Top 10 Most Active Items)\")\n",
    "        print(\"=\" * 120)\n",
    "\n",
    "        for item_name in top_items:\n",
    "            item_runs = completed[completed[\"item_name\"] == item_name].sort_values(\"start_time\")\n",
    "            item_type = item_runs[\"item_type\"].iloc[0]\n",
    "            icon = \"ğŸ““\" if item_type == \"Notebook\" else \"ğŸ”„\"\n",
    "\n",
    "            print(f\"\\n{icon} {item_name} ({len(item_runs)} runs)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            # Weekly aggregation\n",
    "            item_runs[\"week\"] = item_runs[\"start_time\"].dt.isocalendar().week\n",
    "            weekly = item_runs.groupby(\"week\").agg(\n",
    "                runs=(\"duration_seconds\", \"count\"),\n",
    "                avg_dur_min=(\"duration_seconds\", lambda x: x.mean() / 60),\n",
    "                p95_dur_min=(\"duration_seconds\", lambda x: x.quantile(0.95) / 60),\n",
    "            ).reset_index()\n",
    "\n",
    "            for _, row in weekly.iterrows():\n",
    "                bar_len = int(row[\"avg_dur_min\"] * 2)  # Scale bar\n",
    "                bar = \"â–ˆ\" * min(bar_len, 50)\n",
    "                print(f\"   Week {int(row['week']):2d} | \"\n",
    "                      f\"Runs: {int(row['runs']):3d} | \"\n",
    "                      f\"Avg: {row['avg_dur_min']:7.1f} min | \"\n",
    "                      f\"P95: {row['p95_dur_min']:7.1f} min | {bar}\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸  No completed runs with duration data found.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No run data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a3724",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Scheduling & Trigger Analysis\n",
    "\n",
    "Analyzes how Notebooks and Pipelines are invoked â€” manually, on-schedule, or via other triggers.\n",
    "Identifies items that may benefit from scheduling or have irregular execution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Scheduling & Trigger Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_runs.empty:\n",
    "    print(\"ğŸ• SCHEDULING & TRIGGER ANALYSIS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Invoke type distribution\n",
    "    invoke_summary = df_runs.groupby([\"item_type\", \"item_name\", \"invoke_type\"]).size().reset_index(name=\"count\")\n",
    "    invoke_pivot = invoke_summary.pivot_table(\n",
    "        index=[\"item_type\", \"item_name\"],\n",
    "        columns=\"invoke_type\",\n",
    "        values=\"count\",\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"\\nğŸ“Š Invocation Method Distribution\")\n",
    "    print(\"-\" * 100)\n",
    "    print(invoke_pivot.to_string(index=False))\n",
    "\n",
    "    # Execution frequency analysis\n",
    "    print(\"\\n\\nğŸ“… Execution Frequency\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for (item_id, item_name, item_type), group in df_runs.groupby(\n",
    "        [\"item_id\", \"item_name\", \"item_type\"]\n",
    "    ):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        sorted_runs = group.dropna(subset=[\"start_time\"]).sort_values(\"start_time\")\n",
    "        if len(sorted_runs) < 2:\n",
    "            continue\n",
    "\n",
    "        time_diffs = sorted_runs[\"start_time\"].diff().dropna()\n",
    "        avg_interval = time_diffs.mean()\n",
    "        icon = \"ğŸ““\" if item_type == \"Notebook\" else \"ğŸ”„\"\n",
    "\n",
    "        # Determine pattern\n",
    "        if avg_interval <= timedelta(hours=1):\n",
    "            frequency = f\"~{avg_interval.total_seconds()/60:.0f} min (sub-hourly)\"\n",
    "        elif avg_interval <= timedelta(hours=24):\n",
    "            frequency = f\"~{avg_interval.total_seconds()/3600:.1f} hrs (daily pattern)\"\n",
    "        elif avg_interval <= timedelta(days=7):\n",
    "            frequency = f\"~{avg_interval.days} day(s) (weekly pattern)\"\n",
    "        else:\n",
    "            frequency = f\"~{avg_interval.days} day(s) (infrequent)\"\n",
    "\n",
    "        print(f\"   {icon} {item_name}: {frequency}  ({len(sorted_runs)} runs)\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  No run data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565d169",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£b Spark Resource Efficiency (%)\n",
    "\n",
    "Tracks the **Spark Resource Efficiency %** for every notebook in the workspace â€” the same metric\n",
    "shown in the Fabric portal under each notebook run â†’ **Resources** tab â†’ **Spark resource usage**.\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Spark Efficiency %** | `Active Executor-Core-Seconds / Allocated Executor-Core-Seconds Ã— 100` |\n",
    "| **Total Duration** | Wall-clock time the Spark application ran |\n",
    "| **Total Idle Time** | Time allocated cores were idle (not processing tasks) |\n",
    "| **Compute Efficiency** | Successful compute time / Total compute time (from job instances) |\n",
    "| **Duration Consistency** | `P50 / Mean Ã— 100` â€“ higher = less variance across runs |\n",
    "\n",
    "This section tries the **Fabric Spark Applications API** first for real core-level efficiency.\n",
    "If the API is not accessible, it computes a **Compute Efficiency** proxy from job instance data.\n",
    "\n",
    "> **Note:** The real Spark core-utilization efficiency (shown in the portal screenshot) requires\n",
    "> access to the Spark monitoring API. If unavailable, you can view per-run efficiency in the\n",
    "> Fabric portal: open a notebook run â†’ **Resources** tab â†’ **Spark resource usage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f612f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Spark Resource Efficiency Tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def try_get_spark_applications(workspace_id, headers, lookback_days=30):\n",
    "    \"\"\"\n",
    "    Attempt to retrieve Spark application data from Fabric APIs.\n",
    "    Tries the Spark Applications API first, then falls back to Livy Sessions.\n",
    "    Returns (list_of_apps, api_source) or (None, None).\n",
    "    \"\"\"\n",
    "    from_ts = (datetime.now(timezone.utc) - timedelta(days=lookback_days)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # â”€â”€ Attempt 1: Fabric Spark Applications API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    endpoints = [\n",
    "        (f\"{FABRIC_API_BASE}/workspaces/{workspace_id}/spark/applications\",\n",
    "         {\"fromTimestamp\": from_ts}, \"Spark Applications API\"),\n",
    "        (f\"{FABRIC_API_BASE}/workspaces/{workspace_id}/spark/livySessions\",\n",
    "         {}, \"Livy Sessions API\"),\n",
    "    ]\n",
    "\n",
    "    for url, params, source_name in endpoints:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                apps = data.get(\"value\", data.get(\"sessions\", data if isinstance(data, list) else []))\n",
    "                if apps:\n",
    "                    return apps, source_name\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def parse_spark_app_efficiency(apps):\n",
    "    \"\"\"\n",
    "    Parse Spark application data and extract efficiency metrics.\n",
    "    Maps variable field names across different Spark API versions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for app in apps:\n",
    "        name   = app.get(\"name\", app.get(\"appName\", \"Unknown\"))\n",
    "        app_id = app.get(\"id\", app.get(\"appId\", app.get(\"applicationId\", \"\")))\n",
    "        state  = app.get(\"state\", app.get(\"status\", \"\"))\n",
    "\n",
    "        # â”€â”€ Core efficiency metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        total_seconds = app.get(\"totalDuration\", app.get(\"duration\", None))\n",
    "        if total_seconds and isinstance(total_seconds, str):\n",
    "            total_seconds = None  # Must be numeric\n",
    "        idle_seconds  = app.get(\"totalIdleTime\", app.get(\"idleTime\", None))\n",
    "        efficiency    = app.get(\"efficiency\", None)\n",
    "\n",
    "        # Duration from startTime/endTime if totalDuration not available\n",
    "        if total_seconds is None:\n",
    "            start_str = app.get(\"startTime\", app.get(\"startedAt\", \"\"))\n",
    "            end_str   = app.get(\"endTime\", app.get(\"finishedAt\", \"\"))\n",
    "            if start_str and end_str:\n",
    "                try:\n",
    "                    st = parse_datetime(str(start_str))\n",
    "                    et = parse_datetime(str(end_str))\n",
    "                    if st and et:\n",
    "                        total_seconds = (et - st).total_seconds()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Calculate efficiency from components if not directly available\n",
    "        if efficiency is None and total_seconds and idle_seconds:\n",
    "            if total_seconds > 0:\n",
    "                efficiency = ((total_seconds - idle_seconds) / total_seconds) * 100\n",
    "\n",
    "        # Executor core info\n",
    "        total_cores = app.get(\"totalExecutorCores\", app.get(\"executorCores\", None))\n",
    "        executors   = app.get(\"executorCount\", app.get(\"numExecutors\", None))\n",
    "\n",
    "        results.append({\n",
    "            \"notebook_name\":           name,\n",
    "            \"app_id\":                  str(app_id),\n",
    "            \"state\":                   state,\n",
    "            \"total_duration_sec\":      total_seconds,\n",
    "            \"total_idle_sec\":          idle_seconds,\n",
    "            \"spark_efficiency_pct\":    round(efficiency, 2) if efficiency is not None else None,\n",
    "            \"executor_cores\":          total_cores,\n",
    "            \"executor_count\":          executors,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results) if results else pd.DataFrame()\n",
    "\n",
    "\n",
    "def compute_notebook_efficiency_from_jobs(df_runs, notebooks):\n",
    "    \"\"\"\n",
    "    Calculate Compute Efficiency per notebook from job instance data.\n",
    "\n",
    "    Compute Efficiency = Successful Compute Time / Total Compute Time Ã— 100\n",
    "    Consistency        = Duration P50 / Mean Ã— 100 (higher = less variance)\n",
    "    Estimated Efficiency = 70% Compute + 30% Consistency\n",
    "    \"\"\"\n",
    "    if df_runs.empty or not notebooks:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    nb_runs = df_runs[df_runs[\"item_type\"] == \"Notebook\"].copy()\n",
    "    if nb_runs.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results = []\n",
    "    for nb in notebooks:\n",
    "        runs = nb_runs[nb_runs[\"item_id\"] == nb[\"item_id\"]]\n",
    "        if runs.empty:\n",
    "            continue\n",
    "\n",
    "        total_runs  = len(runs)\n",
    "        completed   = runs[runs[\"status\"].str.lower().isin([\"completed\", \"succeeded\"])]\n",
    "        failed      = runs[runs[\"status\"].str.lower().isin([\"failed\", \"cancelled\"])]\n",
    "\n",
    "        all_dur     = runs[\"duration_seconds\"].dropna()\n",
    "        ok_dur      = completed[\"duration_seconds\"].dropna()\n",
    "        fail_dur    = failed[\"duration_seconds\"].dropna()\n",
    "\n",
    "        total_sec   = all_dur.sum()\n",
    "        ok_sec      = ok_dur.sum()\n",
    "        wasted_sec  = fail_dur.sum()\n",
    "        avg_dur_sec = all_dur.mean() if len(all_dur) > 0 else 0\n",
    "        idle_est_sec = wasted_sec  # Wasted compute is analogous to idle time\n",
    "\n",
    "        # Compute Efficiency\n",
    "        compute_eff = (ok_sec / total_sec * 100) if total_sec > 0 else 0\n",
    "\n",
    "        # Consistency = P50 / Mean (higher = runs are consistent in duration)\n",
    "        consistency = 100.0\n",
    "        if len(ok_dur) >= 2:\n",
    "            p50  = ok_dur.median()\n",
    "            mean = ok_dur.mean()\n",
    "            consistency = min((p50 / mean * 100) if mean > 0 else 100, 100)\n",
    "\n",
    "        # Estimated Spark Efficiency (weighted proxy)\n",
    "        estimated_eff = compute_eff * 0.7 + consistency * 0.3\n",
    "\n",
    "        results.append({\n",
    "            \"item_name\":                nb[\"item_name\"],\n",
    "            \"total_runs\":               total_runs,\n",
    "            \"completed_runs\":           len(completed),\n",
    "            \"failed_runs\":              len(failed),\n",
    "            \"total_compute_min\":        round(total_sec / 60, 1),\n",
    "            \"successful_compute_min\":   round(ok_sec / 60, 1),\n",
    "            \"wasted_compute_min\":       round(wasted_sec / 60, 1),\n",
    "            \"avg_duration_min\":         round(avg_dur_sec / 60, 1),\n",
    "            \"idle_estimate_min\":        round(idle_est_sec / 60, 1),\n",
    "            \"compute_efficiency_pct\":   round(compute_eff, 1),\n",
    "            \"consistency_pct\":          round(consistency, 1),\n",
    "            \"estimated_efficiency_pct\": round(estimated_eff, 1),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"estimated_efficiency_pct\") if results else pd.DataFrame()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MAIN: Spark Resource Efficiency\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"âš¡ SPARK RESOURCE EFFICIENCY\")\n",
    "print(\"=\" * 130)\n",
    "\n",
    "df_spark_api = pd.DataFrame()  # Holds real Spark data (if available)\n",
    "df_spark_eff = pd.DataFrame()  # Holds estimated efficiency (always calculated)\n",
    "\n",
    "if INCLUDE_NOTEBOOKS and notebooks:\n",
    "\n",
    "    # â”€â”€ Try Fabric Spark API for real core-utilization efficiency â”€â”€\n",
    "    spark_apps, spark_source = try_get_spark_applications(WORKSPACE_ID, HEADERS, LOOKBACK_DAYS)\n",
    "\n",
    "    if spark_apps:\n",
    "        df_spark_api = parse_spark_app_efficiency(spark_apps)\n",
    "        if not df_spark_api.empty:\n",
    "            print(f\"âœ… Real Spark efficiency data retrieved via {spark_source}\")\n",
    "            print(f\"   Applications found: {len(df_spark_api)}\\n\")\n",
    "\n",
    "            eff_col = \"spark_efficiency_pct\"\n",
    "            apps_with_eff = df_spark_api[df_spark_api[eff_col].notna()]\n",
    "            apps_no_eff   = df_spark_api[df_spark_api[eff_col].isna()]\n",
    "\n",
    "            if not apps_with_eff.empty:\n",
    "                for _, row in apps_with_eff.iterrows():\n",
    "                    eff = row[eff_col]\n",
    "                    icon = \"ğŸŸ¢\" if eff >= 70 else (\"ğŸŸ¡\" if eff >= 50 else \"ğŸ”´\")\n",
    "                    dur  = f\"{row['total_duration_sec']/60:.1f} min\" if row.get(\"total_duration_sec\") else \"N/A\"\n",
    "                    idle = f\"{row['total_idle_sec']/60:.1f} min\" if row.get(\"total_idle_sec\") else \"N/A\"\n",
    "                    print(f\"   {icon} {row['notebook_name']}\")\n",
    "                    print(f\"      Duration: {dur}  |  Idle: {idle}  |  Efficiency: {eff:.1f}%\")\n",
    "\n",
    "            if not apps_no_eff.empty:\n",
    "                print(f\"\\n   â„¹ï¸  {len(apps_no_eff)} app(s) without core-level efficiency data\")\n",
    "\n",
    "    else:\n",
    "        print(\"â„¹ï¸  Spark monitoring API not available via REST.\")\n",
    "        print(\"   ğŸ“Œ For real core-utilization efficiency, open individual notebook runs in Fabric portal:\")\n",
    "        print(\"      Notebook run â†’ Resources tab â†’ Spark resource usage\")\n",
    "\n",
    "    # â”€â”€ Always compute estimated efficiency from job instances â”€â”€â”€â”€â”€\n",
    "    if not df_runs.empty:\n",
    "        df_spark_eff = compute_notebook_efficiency_from_jobs(df_runs, notebooks)\n",
    "\n",
    "    if not df_spark_eff.empty:\n",
    "        print(f\"\\nğŸ“Š NOTEBOOK COMPUTE EFFICIENCY DASHBOARD\")\n",
    "        print(\"=\" * 140)\n",
    "        header = (f\"   {'Notebook':<40s} {'Runs':>5s} {'Pass':>5s} {'Fail':>5s} \"\n",
    "                  f\"{'Total(min)':>10s} {'OK(min)':>8s} {'Wasted(min)':>11s} \"\n",
    "                  f\"{'Compute%':>9s} {'Consist%':>9s} {'Effic%':>7s}\")\n",
    "        print(header)\n",
    "        print(\"   \" + \"-\" * 134)\n",
    "\n",
    "        for _, row in df_spark_eff.iterrows():\n",
    "            eff = row[\"estimated_efficiency_pct\"]\n",
    "            icon = \"ğŸŸ¢\" if eff >= 90 else (\"ğŸŸ¡\" if eff >= 70 else \"ğŸ”´\")\n",
    "            print(f\"   {icon} {row['item_name']:<38s} \"\n",
    "                  f\"{row['total_runs']:>5d} {row['completed_runs']:>5d} {row['failed_runs']:>5d} \"\n",
    "                  f\"{row['total_compute_min']:>10.1f} {row['successful_compute_min']:>8.1f} \"\n",
    "                  f\"{row['wasted_compute_min']:>11.1f} \"\n",
    "                  f\"{row['compute_efficiency_pct']:>8.1f}% {row['consistency_pct']:>8.1f}% \"\n",
    "                  f\"{row['estimated_efficiency_pct']:>6.1f}%\")\n",
    "\n",
    "        avg_eff      = df_spark_eff[\"estimated_efficiency_pct\"].mean()\n",
    "        total_wasted = df_spark_eff[\"wasted_compute_min\"].sum()\n",
    "        total_compute = df_spark_eff[\"total_compute_min\"].sum()\n",
    "        print(\"   \" + \"-\" * 134)\n",
    "        print(f\"   Average Efficiency: {avg_eff:.1f}%  |  \"\n",
    "              f\"Total Compute: {total_compute:,.1f} min  |  Wasted: {total_wasted:,.1f} min\")\n",
    "\n",
    "        # â”€â”€ Efficiency tier breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        high_eff = len(df_spark_eff[df_spark_eff[\"estimated_efficiency_pct\"] >= 90])\n",
    "        med_eff  = len(df_spark_eff[(df_spark_eff[\"estimated_efficiency_pct\"] >= 70) &\n",
    "                                    (df_spark_eff[\"estimated_efficiency_pct\"] < 90)])\n",
    "        low_eff  = len(df_spark_eff[df_spark_eff[\"estimated_efficiency_pct\"] < 70])\n",
    "\n",
    "        print(f\"\\n   ğŸŸ¢ Efficient (â‰¥90%):   {high_eff} notebook(s)\")\n",
    "        print(f\"   ğŸŸ¡ Moderate (70-89%):  {med_eff} notebook(s)\")\n",
    "        print(f\"   ğŸ”´ Low (<70%):         {low_eff} notebook(s)\")\n",
    "\n",
    "        # â”€â”€ Recommendations for low-efficiency notebooks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        low_eff_items = df_spark_eff[df_spark_eff[\"estimated_efficiency_pct\"] < 70]\n",
    "        if not low_eff_items.empty:\n",
    "            print(f\"\\nğŸ’¡ LOW EFFICIENCY NOTEBOOKS â€“ ACTION REQUIRED\")\n",
    "            print(\"-\" * 80)\n",
    "            for _, row in low_eff_items.iterrows():\n",
    "                print(f\"   ğŸ”´ '{row['item_name']}'  â†’  Efficiency: {row['estimated_efficiency_pct']:.1f}%\")\n",
    "                if row[\"failed_runs\"] > 0:\n",
    "                    print(f\"      âš ï¸  {row['failed_runs']} failed runs wasted {row['wasted_compute_min']:.1f} min of compute\")\n",
    "                if row[\"consistency_pct\"] < 70:\n",
    "                    print(f\"      âš ï¸  Duration variance is high (Consistency: {row['consistency_pct']:.1f}%) â€“ \"\n",
    "                          \"investigate outlier runs\")\n",
    "\n",
    "        # â”€â”€ Metric definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(f\"\\nğŸ“‹ METRIC DEFINITIONS\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"   Compute %:     Successful compute time / Total compute time Ã— 100\")\n",
    "        print(\"   Consistency %: Duration P50 / Mean Ã— 100 (higher = runs are uniform)\")\n",
    "        print(\"   Efficiency %:  Weighted: 70% Compute + 30% Consistency\")\n",
    "        print()\n",
    "        print(\"   ğŸ’¡ For real Spark core-utilization efficiency (executor idle time),\")\n",
    "        print(\"      open individual notebook runs â†’ Resources tab â†’ Spark resource usage\")\n",
    "    else:\n",
    "        print(\"   â„¹ï¸  No notebook run data available for efficiency calculation.\")\n",
    "\n",
    "else:\n",
    "    print(\"   â­ï¸  Skipped â€“ notebook tracking is disabled or no notebooks found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116821bd",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Consolidated Summary\n",
    "\n",
    "High-level summary comparing Notebooks vs Pipelines efficiency across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Consolidated Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_efficiency.empty:\n",
    "    print(\"ğŸ“Š CONSOLIDATED EFFICIENCY SUMMARY\")\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    for itype in [\"Notebook\", \"DataPipeline\"]:\n",
    "        subset = df_efficiency[df_efficiency[\"item_type\"] == itype]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        icon = \"ğŸ““\" if itype == \"Notebook\" else \"ğŸ”„\"\n",
    "        label = \"Notebooks\" if itype == \"Notebook\" else \"Data Pipelines\"\n",
    "\n",
    "        total_items = len(subset)\n",
    "        total_runs = subset[\"total_runs\"].sum()\n",
    "        total_success = subset[\"success_count\"].sum()\n",
    "        total_failures = subset[\"failure_count\"].sum()\n",
    "        overall_rate = (total_success / total_runs * 100) if total_runs > 0 else 0\n",
    "        total_compute = subset[\"total_compute_hrs\"].sum()\n",
    "        avg_duration_all = subset[\"avg_duration_min\"].mean() if subset[\"avg_duration_min\"].notna().any() else 0\n",
    "\n",
    "        print(f\"\\n{icon} {label}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"   Items:              {total_items}\")\n",
    "        print(f\"   Total Runs:         {total_runs:,}\")\n",
    "        print(f\"   Successful:         {total_success:,}\")\n",
    "        print(f\"   Failed/Cancelled:   {total_failures:,}\")\n",
    "        print(f\"   Success Rate:       {overall_rate:.1f}%\")\n",
    "        print(f\"   Avg Duration:       {avg_duration_all:.1f} min\")\n",
    "        print(f\"   Total Compute:      {total_compute:,.1f} hours\")\n",
    "\n",
    "        # â”€â”€ Spark Efficiency summary (Notebooks only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if itype == \"Notebook\" and 'df_spark_eff' in dir() and not df_spark_eff.empty:\n",
    "            avg_eff      = df_spark_eff[\"estimated_efficiency_pct\"].mean()\n",
    "            wasted_min   = df_spark_eff[\"wasted_compute_min\"].sum()\n",
    "            low_count    = len(df_spark_eff[df_spark_eff[\"estimated_efficiency_pct\"] < 70])\n",
    "            print(f\"   âš¡ Avg Spark Efficiency: {avg_eff:.1f}%\")\n",
    "            print(f\"   âš¡ Wasted Compute:       {wasted_min:,.1f} min\")\n",
    "            if low_count > 0:\n",
    "                print(f\"   âš¡ Low Efficiency (<70%): {low_count} notebook(s)\")\n",
    "\n",
    "        # Top 3 longest running\n",
    "        top3 = subset.nlargest(3, \"avg_duration_min\")\n",
    "        if not top3.empty:\n",
    "            print(f\"   ğŸ¢ Slowest Items:\")\n",
    "            for _, row in top3.iterrows():\n",
    "                print(f\"      - {row['item_name']}: avg {row['avg_duration_min']:.1f} min\")\n",
    "\n",
    "        # Top 3 most failing\n",
    "        top3_fail = subset[subset[\"failure_count\"] > 0].nlargest(3, \"failure_count\")\n",
    "        if not top3_fail.empty:\n",
    "            print(f\"   âŒ Most Failing:\")\n",
    "            for _, row in top3_fail.iterrows():\n",
    "                print(f\"      - {row['item_name']}: {row['failure_count']} failures ({row['success_rate_pct']}% success)\")\n",
    "\n",
    "        # Top 3 lowest efficiency (Notebooks only)\n",
    "        if itype == \"Notebook\" and 'df_spark_eff' in dir() and not df_spark_eff.empty:\n",
    "            low3 = df_spark_eff.nsmallest(3, \"estimated_efficiency_pct\")\n",
    "            if not low3.empty:\n",
    "                print(f\"   âš¡ Least Efficient:\")\n",
    "                for _, row in low3.iterrows():\n",
    "                    print(f\"      - {row['item_name']}: {row['estimated_efficiency_pct']:.1f}% efficiency \"\n",
    "                          f\"({row['wasted_compute_min']:.1f} min wasted)\")\n",
    "\n",
    "    # â”€â”€ Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\\nğŸ’¡ RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    # Items with low success rate\n",
    "    low_success = df_efficiency[df_efficiency[\"success_rate_pct\"] < 80]\n",
    "    if not low_success.empty:\n",
    "        for _, row in low_success.iterrows():\n",
    "            recs.append(f\"ğŸ”´ '{row['item_name']}' has {row['success_rate_pct']}% success rate â€“ investigate failures\")\n",
    "\n",
    "    # Items with degrading trend\n",
    "    degrading = df_efficiency[df_efficiency[\"duration_trend\"].str.contains(\"Degrading\", na=False)]\n",
    "    if not degrading.empty:\n",
    "        for _, row in degrading.iterrows():\n",
    "            recs.append(f\"â¬‡ï¸ '{row['item_name']}' shows performance degradation â€“ review recent changes\")\n",
    "\n",
    "    # Items with very long P95\n",
    "    if df_efficiency[\"p95_duration_min\"].notna().any():\n",
    "        long_p95 = df_efficiency[df_efficiency[\"p95_duration_min\"] > df_efficiency[\"avg_duration_min\"] * 3]\n",
    "        long_p95 = long_p95[long_p95[\"p95_duration_min\"].notna()]\n",
    "        for _, row in long_p95.iterrows():\n",
    "            recs.append(f\"ğŸ“Š '{row['item_name']}' has high P95 variance (avg: {row['avg_duration_min']:.1f} min, \"\n",
    "                       f\"P95: {row['p95_duration_min']:.1f} min) â€“ consider investigating outlier runs\")\n",
    "\n",
    "    # Low Spark efficiency notebooks\n",
    "    if 'df_spark_eff' in dir() and not df_spark_eff.empty:\n",
    "        low_eff_items = df_spark_eff[df_spark_eff[\"estimated_efficiency_pct\"] < 70]\n",
    "        for _, row in low_eff_items.iterrows():\n",
    "            recs.append(f\"âš¡ '{row['item_name']}' has low compute efficiency ({row['estimated_efficiency_pct']:.1f}%) â€“ \"\n",
    "                       f\"{row['wasted_compute_min']:.1f} min wasted on failed runs\")\n",
    "\n",
    "    if recs:\n",
    "        for i, rec in enumerate(recs, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "    else:\n",
    "        print(\"   âœ… All items are performing well. No immediate action required.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No data to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a03976",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Export Report to CSV\n",
    "\n",
    "Saves the full efficiency report and detailed run history to CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37091140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export to CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# â”€â”€ Export efficiency summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not df_efficiency.empty:\n",
    "    summary_file = f\"fabric_efficiency_summary_{timestamp}.csv\"\n",
    "    export_cols = [\n",
    "        \"item_type\", \"item_name\", \"total_runs\", \"success_count\", \"failure_count\",\n",
    "        \"success_rate_pct\", \"avg_duration_min\", \"p50_duration_min\", \"p95_duration_min\",\n",
    "        \"min_duration_min\", \"max_duration_min\", \"total_compute_hrs\",\n",
    "        \"last_run_time\", \"last_status\", \"duration_trend\", \"top_failure_reason\"\n",
    "    ]\n",
    "    df_efficiency[[c for c in export_cols if c in df_efficiency.columns]].to_csv(\n",
    "        summary_file, index=False\n",
    "    )\n",
    "    print(f\"âœ… Efficiency summary exported to: {summary_file}\")\n",
    "    print(f\"   Rows: {len(df_efficiency)}\")\n",
    "\n",
    "# â”€â”€ Export detailed run history â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not df_runs.empty:\n",
    "    runs_file = f\"fabric_run_history_{timestamp}.csv\"\n",
    "    df_runs_export = df_runs.copy()\n",
    "    # Convert datetime for CSV compatibility\n",
    "    for col in [\"start_time\", \"end_time\"]:\n",
    "        if col in df_runs_export.columns:\n",
    "            df_runs_export[col] = df_runs_export[col].astype(str)\n",
    "    df_runs_export.to_csv(runs_file, index=False)\n",
    "    print(f\"âœ… Run history exported to: {runs_file}\")\n",
    "    print(f\"   Rows: {len(df_runs_export)}\")\n",
    "\n",
    "# â”€â”€ Export Spark efficiency (Notebooks) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if 'df_spark_eff' in dir() and not df_spark_eff.empty:\n",
    "    spark_file = f\"fabric_spark_efficiency_{timestamp}.csv\"\n",
    "    df_spark_eff.to_csv(spark_file, index=False)\n",
    "    print(f\"âœ… Spark efficiency exported to: {spark_file}\")\n",
    "    print(f\"   Rows: {len(df_spark_eff)}\")\n",
    "\n",
    "# â”€â”€ Export real Spark API data (if available) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if 'df_spark_api' in dir() and not df_spark_api.empty:\n",
    "    spark_api_file = f\"fabric_spark_api_data_{timestamp}.csv\"\n",
    "    df_spark_api.to_csv(spark_api_file, index=False)\n",
    "    print(f\"âœ… Spark API data exported to: {spark_api_file}\")\n",
    "    print(f\"   Rows: {len(df_spark_api)}\")\n",
    "\n",
    "if df_efficiency.empty and df_runs.empty:\n",
    "    print(\"âš ï¸  No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568f6e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Notes\n",
    "\n",
    "### Running This Notebook\n",
    "- **Import** into any Fabric workspace via the Fabric portal\n",
    "- **Attach a Lakehouse** (optional) to persist CSV exports as files\n",
    "- Set `TARGET_WORKSPACE_ID` to scan a different workspace, or leave `None` for the current one\n",
    "- Adjust `LOOKBACK_DAYS` to control the analysis window (default: 30 days)\n",
    "\n",
    "### Fabric REST API Endpoints Used\n",
    "\n",
    "| Endpoint | Purpose |\n",
    "|----------|---------|\n",
    "| `GET /v1/workspaces/{id}` | Workspace metadata |\n",
    "| `GET /v1/workspaces/{id}/items?type=Notebook` | List all notebooks |\n",
    "| `GET /v1/workspaces/{id}/items?type=DataPipeline` | List all data pipelines |\n",
    "| `GET /v1/workspaces/{id}/items/{id}/jobs/instances` | Run history per item |\n",
    "| `GET /v1/workspaces/{id}/spark/applications` | Spark application metrics (if available) |\n",
    "| `GET /v1/workspaces/{id}/spark/livySessions` | Livy session data (fallback) |\n",
    "\n",
    "### Spark Resource Efficiency\n",
    "\n",
    "The **Spark Resource Efficiency %** shown in the Fabric portal (Resources tab) measures how effectively\n",
    "Spark executor cores are utilized:\n",
    "\n",
    "```\n",
    "Efficiency = Active Executor-Core-Seconds / Allocated Executor-Core-Seconds Ã— 100\n",
    "```\n",
    "\n",
    "This notebook attempts to retrieve this real efficiency via the Fabric Spark API. If the API is not\n",
    "accessible, it calculates a **Compute Efficiency** proxy:\n",
    "\n",
    "| Metric | Formula | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Compute %** | `Successful_Time / Total_Time Ã— 100` | How much compute was productive vs. wasted on failures |\n",
    "| **Consistency %** | `Duration_P50 / Duration_Mean Ã— 100` | How uniform run durations are (higher = less variance) |\n",
    "| **Efficiency %** | `70% Ã— Compute + 30% Ã— Consistency` | Combined proxy for overall Spark efficiency |\n",
    "\n",
    "To view the **real** Spark core-utilization efficiency per run:\n",
    "1. Open a notebook run in the Fabric portal\n",
    "2. Click the **Resources** tab\n",
    "3. View **Spark resource usage** â†’ Total duration / Idle time / Efficiency %\n",
    "\n",
    "### Metrics Explained\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Success Rate** | `successful_runs / total_runs Ã— 100` |\n",
    "| **Avg Duration** | Mean execution time across all completed runs |\n",
    "| **P50 (Median)** | 50th percentile â€“ half the runs are faster |\n",
    "| **P95** | 95th percentile â€“ 95% of runs are faster |\n",
    "| **Total Compute** | Cumulative hours of execution time consumed |\n",
    "| **Trend** | Compares avg duration of recent runs vs older runs |\n",
    "| **Wasted Compute** | Time consumed by failed/cancelled runs |\n",
    "\n",
    "### Health Thresholds\n",
    "\n",
    "| Status | Criteria | Action |\n",
    "|--------|----------|--------|\n",
    "| ğŸŸ¢ Healthy | â‰¥ 95% success rate | No action needed |\n",
    "| ğŸŸ¡ Warning | 80â€“94% success rate | Monitor and investigate |\n",
    "| ğŸ”´ Critical | < 80% success rate | Immediate investigation required |\n",
    "\n",
    "### Spark Efficiency Thresholds\n",
    "\n",
    "| Status | Criteria | Action |\n",
    "|--------|----------|--------|\n",
    "| ğŸŸ¢ Efficient | â‰¥ 90% efficiency | Well-optimized |\n",
    "| ğŸŸ¡ Moderate | 70â€“89% efficiency | Review for optimization opportunities |\n",
    "| ğŸ”´ Low | < 70% efficiency | Investigate failures, duration variance, and Spark resource usage |\n",
    "\n",
    "### Permissions Required\n",
    "- **Workspace Admin** or **Member** role for full access to job instances\n",
    "- **Viewer** role may have limited access to run history\n",
    "\n",
    "### Limitations\n",
    "- Run history retention depends on your Fabric capacity tier\n",
    "- Real Spark core-utilization efficiency requires the Spark Applications API (availability may vary)\n",
    "- CU (Capacity Unit) consumption details require Admin API access\n",
    "- Very long-running pipelines with many activities may have truncated details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
