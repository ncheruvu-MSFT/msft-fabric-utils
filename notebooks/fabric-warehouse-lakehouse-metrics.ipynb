{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a07cef",
   "metadata": {},
   "source": [
    "# ğŸ“Š Microsoft Fabric â€“ Warehouse & Lakehouse Metrics\n",
    "\n",
    "This notebook queries **Microsoft Fabric Warehouses and Lakehouses** to collect table-level metrics:\n",
    "\n",
    "| Metric | Source |\n",
    "|--------|--------|\n",
    "| Warehouse / Lakehouse Name | Fabric REST API |\n",
    "| Table Name | SQL Endpoint DMVs / REST API |\n",
    "| Number of Rows | SQL Endpoint DMVs |\n",
    "| Size on Disk | SQL Endpoint DMVs |\n",
    "| Maintenance On/Off | Fabric REST API (table maintenance) |\n",
    "| Maintenance Retention (Days) | Fabric REST API |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"\\ud83d\\udd10 Fabric Auth\"] --> B[\"\\ud83d\\udce1 Fabric REST API\"]\n",
    "    B --> C[\"\\ud83d\\udce6 List Warehouses\"]\n",
    "    B --> D[\"\\ud83c\\udfe0 List Lakehouses\"]\n",
    "    C --> E[\"\\ud83d\\udd17 SQL Endpoint\"]\n",
    "    D --> F[\"\\ud83d\\udd17 SQL Endpoint\"]\n",
    "    E --> G[\"\\ud83d\\udcca Table Metrics\"]\n",
    "    F --> G\n",
    "    G --> H[\"\\ud83d\\udccb Consolidated Report\"]\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- This notebook runs **inside Microsoft Fabric** (not externally)\n",
    "- Fabric workspace with Warehouse or Lakehouse items\n",
    "- Pre-installed libraries: `sempy`, `mssparkutils`, `pandas`, `requests`, `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a69cd",
   "metadata": {},
   "source": [
    "## 0ï¸âƒ£ Verify Fabric Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify Fabric Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# All libraries below are pre-installed in the Fabric PySpark runtime.\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "from notebookutils import mssparkutils\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Fabric environment verified\")\n",
    "print(f\"   sempy version:      {fabric.__version__ if hasattr(fabric, '__version__') else 'OK'}\")\n",
    "print(f\"   mssparkutils:       available\")\n",
    "print(f\"   pandas version:     {pd.__version__}\")\n",
    "print(f\"   spark:              {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f03779",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Configuration\n",
    "\n",
    "Set your Fabric workspace ID and optional filters below.  \n",
    "You can find your **Workspace ID** in the Fabric portal URL:  \n",
    "`https://app.fabric.microsoft.com/groups/<WORKSPACE_ID>/...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ Fabric Workspace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to None to use the current workspace, or specify a workspace GUID\n",
    "TARGET_WORKSPACE_ID = None   # e.g. \"a1b2c3d4-e5f6-7890-abcd-1234567890ef\"\n",
    "\n",
    "# â”€â”€ Resolve workspace ID â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if TARGET_WORKSPACE_ID:\n",
    "    WORKSPACE_ID = TARGET_WORKSPACE_ID\n",
    "else:\n",
    "    WORKSPACE_ID = fabric.get_workspace_id()\n",
    "\n",
    "# â”€â”€ Fabric API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "# â”€â”€ Authentication via mssparkutils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fabric_token = mssparkutils.credentials.getToken(\"pbi\")\n",
    "sql_token    = mssparkutils.credentials.getToken(\"https://database.windows.net\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {fabric_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# â”€â”€ Display Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# â”€â”€ Validate â€“ fetch workspace info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "resp = requests.get(f\"{FABRIC_API_BASE}/workspaces/{WORKSPACE_ID}\", headers=HEADERS)\n",
    "resp.raise_for_status()\n",
    "ws = resp.json()\n",
    "print(f\"ğŸ“Š Fabric Metrics Configuration\")\n",
    "print(f\"   Workspace:    {ws['displayName']}\")\n",
    "print(f\"   Workspace ID: {WORKSPACE_ID}\")\n",
    "print(f\"   Capacity:     {ws.get('capacityId', 'N/A')}\")\n",
    "print(f\"   API Base:     {FABRIC_API_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696a039",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Discover Warehouses & Lakehouses\n",
    "\n",
    "Uses the Fabric REST API to list all Warehouses and Lakehouses in the workspace, including their SQL analytics endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868db21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Discover Warehouses & Lakehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def list_fabric_items(workspace_id, item_type, headers):\n",
    "    \"\"\"List all items of a given type in a Fabric workspace (handles pagination).\"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/workspaces/{workspace_id}/{item_type}\"\n",
    "    items = []\n",
    "    while url:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items.extend(data.get(\"value\", []))\n",
    "        url = data.get(\"continuationUri\")\n",
    "    return items\n",
    "\n",
    "# â”€â”€ Warehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "raw_warehouses = list_fabric_items(WORKSPACE_ID, \"warehouses\", HEADERS)\n",
    "warehouses = []\n",
    "for w in raw_warehouses:\n",
    "    props = w.get(\"properties\", {})\n",
    "    conn_string = props.get(\"connectionString\", \"\")\n",
    "    sql_ep = conn_string.replace(\"jdbc:sqlserver://\", \"\").split(\";\")[0] if conn_string else \"\"\n",
    "    warehouses.append({\n",
    "        \"item_id\":       w[\"id\"],\n",
    "        \"item_name\":     w[\"displayName\"],\n",
    "        \"item_type\":     \"Warehouse\",\n",
    "        \"sql_endpoint\":  sql_ep,\n",
    "        \"database_name\": w[\"displayName\"],\n",
    "    })\n",
    "print(f\"ğŸ“¦ Found {len(warehouses)} Warehouse(s)\")\n",
    "\n",
    "# â”€â”€ Lakehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "raw_lakehouses = list_fabric_items(WORKSPACE_ID, \"lakehouses\", HEADERS)\n",
    "lakehouses = []\n",
    "for lh in raw_lakehouses:\n",
    "    props = lh.get(\"properties\", {})\n",
    "    sql_ep = props.get(\"sqlEndpointProperties\", {}).get(\"connectionString\", \"\")\n",
    "    lakehouses.append({\n",
    "        \"item_id\":       lh[\"id\"],\n",
    "        \"item_name\":     lh[\"displayName\"],\n",
    "        \"item_type\":     \"Lakehouse\",\n",
    "        \"sql_endpoint\":  sql_ep,\n",
    "        \"database_name\": lh[\"displayName\"],\n",
    "    })\n",
    "print(f\"ğŸ  Found {len(lakehouses)} Lakehouse(s)\")\n",
    "\n",
    "# â”€â”€ Combined list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_items = warehouses + lakehouses\n",
    "print(f\"\\nâœ… Total: {len(all_items)} item(s) to scan\")\n",
    "for item in all_items:\n",
    "    print(f\"   {item['item_type']:10s} | {item['item_name']} | SQL: {item['sql_endpoint'][:60] if item['sql_endpoint'] else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d0cc5",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Query Table Metrics via SQL Endpoint\n",
    "\n",
    "Connects to each Warehouse/Lakehouse SQL analytics endpoint using **Spark JDBC** (always available in Fabric).  \n",
    "Retrieves per-table: **row count**, **reserved size on disk**, **used size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaccfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Query Table Metrics via SQL Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uses Spark's built-in JDBC connector (always available in Fabric)\n",
    "# with the mssparkutils token for authentication.\n",
    "#\n",
    "# NOTE: Fabric SQL endpoints do NOT have sys.dm_pdw_nodes_db_partition_stats\n",
    "# (that's a Synapse dedicated SQL pool DMV). We use:\n",
    "#   - Primary:  sys.dm_db_partition_stats  (row counts + page sizes)\n",
    "#   - Fallback: sys.partitions             (row counts only, no size)\n",
    "\n",
    "# Primary query â€“ uses sys.dm_db_partition_stats for rows + size\n",
    "TABLE_METRICS_QUERY_PRIMARY = \"\"\"(\n",
    "SELECT\n",
    "    s.name                          AS schema_name,\n",
    "    t.name                          AS table_name,\n",
    "    t.create_date                   AS created_date,\n",
    "    t.modify_date                   AS modified_date,\n",
    "    SUM(ps.row_count)               AS row_count,\n",
    "    SUM(ps.reserved_page_count) * 8 AS reserved_size_kb,\n",
    "    SUM(ps.used_page_count) * 8     AS used_size_kb\n",
    "FROM sys.tables t\n",
    "INNER JOIN sys.schemas s\n",
    "    ON t.schema_id = s.schema_id\n",
    "INNER JOIN sys.dm_db_partition_stats ps\n",
    "    ON t.object_id = ps.object_id\n",
    "WHERE ps.index_id IN (0, 1)\n",
    "    AND t.is_ms_shipped = 0\n",
    "GROUP BY s.name, t.name, t.create_date, t.modify_date\n",
    ") AS tbl\"\"\"\n",
    "\n",
    "# Fallback query â€“ uses sys.partitions (always available, no size info)\n",
    "TABLE_METRICS_QUERY_FALLBACK = \"\"\"(\n",
    "SELECT\n",
    "    s.name              AS schema_name,\n",
    "    t.name              AS table_name,\n",
    "    t.create_date       AS created_date,\n",
    "    t.modify_date       AS modified_date,\n",
    "    SUM(p.rows)         AS row_count,\n",
    "    CAST(-1 AS BIGINT)  AS reserved_size_kb,\n",
    "    CAST(-1 AS BIGINT)  AS used_size_kb\n",
    "FROM sys.tables t\n",
    "INNER JOIN sys.schemas s\n",
    "    ON t.schema_id = s.schema_id\n",
    "INNER JOIN sys.partitions p\n",
    "    ON t.object_id = p.object_id\n",
    "WHERE p.index_id IN (0, 1)\n",
    "    AND t.is_ms_shipped = 0\n",
    "GROUP BY s.name, t.name, t.create_date, t.modify_date\n",
    ") AS tbl\"\"\"\n",
    "\n",
    "\n",
    "def query_sql_endpoint(sql_endpoint, database_name, access_token, query):\n",
    "    \"\"\"Query a Fabric SQL endpoint using Spark JDBC.\"\"\"\n",
    "    jdbc_url = (\n",
    "        f\"jdbc:sqlserver://{sql_endpoint};\"\n",
    "        f\"database={database_name};\"\n",
    "        f\"encrypt=true;\"\n",
    "        f\"trustServerCertificate=false;\"\n",
    "        f\"loginTimeout=30;\"\n",
    "    )\n",
    "    spark_df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", query)\n",
    "        .option(\"accessToken\", access_token)\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "        .load()\n",
    "    )\n",
    "    return spark_df.toPandas()\n",
    "\n",
    "\n",
    "def query_with_fallback(sql_endpoint, database_name, access_token):\n",
    "    \"\"\"Try primary query (with size), fall back to partitions-only (no size).\"\"\"\n",
    "    try:\n",
    "        df = query_sql_endpoint(sql_endpoint, database_name, access_token, TABLE_METRICS_QUERY_PRIMARY)\n",
    "        return df, \"dm_db_partition_stats\"\n",
    "    except Exception as e1:\n",
    "        print(f\"      â„¹ï¸  dm_db_partition_stats not available, trying sys.partitions ...\")\n",
    "        try:\n",
    "            df = query_sql_endpoint(sql_endpoint, database_name, access_token, TABLE_METRICS_QUERY_FALLBACK)\n",
    "            return df, \"partitions\"\n",
    "        except Exception as e2:\n",
    "            raise Exception(f\"Both queries failed.\\n  Primary: {e1}\\n  Fallback: {e2}\")\n",
    "\n",
    "\n",
    "# â”€â”€ Collect metrics from all items â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_metrics = []\n",
    "\n",
    "for item in all_items:\n",
    "    sql_ep    = item[\"sql_endpoint\"]\n",
    "    db_name   = item[\"database_name\"]\n",
    "    item_name = item[\"item_name\"]\n",
    "    item_type = item[\"item_type\"]\n",
    "\n",
    "    if not sql_ep:\n",
    "        print(f\"âš ï¸  Skipping {item_type} '{item_name}' â€“ no SQL endpoint found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ğŸ”— Connecting to {item_type}: {item_name} ...\")\n",
    "    try:\n",
    "        df_tables, source = query_with_fallback(sql_ep, db_name, sql_token)\n",
    "        df_tables[\"item_type\"] = item_type\n",
    "        df_tables[\"item_name\"] = item_name\n",
    "        df_tables[\"item_id\"]   = item[\"item_id\"]\n",
    "        all_metrics.append(df_tables)\n",
    "\n",
    "        print(f\"   âœ… Retrieved {len(df_tables)} table(s)  [via {source}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error querying {item_name}: {e}\")\n",
    "\n",
    "if all_metrics:\n",
    "    df_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "    print(f\"\\nğŸ“Š Total: {len(df_metrics)} table(s) across {len(all_metrics)} item(s)\")\n",
    "else:\n",
    "    df_metrics = pd.DataFrame()\n",
    "    print(\"\\nâš ï¸  No table metrics collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b0f2d",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Query Table Maintenance Settings\n",
    "\n",
    "Fabric supports **automatic table maintenance** for Warehouses and Lakehouses:  \n",
    "- **V-Order optimization** â€“ reorganizes data for better compression & read performance  \n",
    "- **File compaction** â€“ merges small Parquet/Delta files  \n",
    "- **Unreferenced file removal** â€“ cleans orphaned files  \n",
    "\n",
    "This cell queries the Fabric REST API for maintenance configuration per item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Query Table Maintenance Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def get_maintenance_settings(workspace_id, item_id, item_type):\n",
    "    \"\"\"\n",
    "    Retrieve table maintenance settings for a Warehouse or Lakehouse.\n",
    "    Returns dict with maintenance_enabled, retention_days, and details.\n",
    "    \"\"\"\n",
    "    maintenance_info = {\n",
    "        \"maintenance_enabled\": \"Unknown\",\n",
    "        \"retention_period_days\": None,\n",
    "        \"vorder_enabled\": \"Unknown\",\n",
    "        \"compaction_enabled\": \"Unknown\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if item_type == \"Lakehouse\":\n",
    "            url = (f\"{FABRIC_API_BASE}/workspaces/{workspace_id}\"\n",
    "                   f\"/lakehouses/{item_id}\")\n",
    "            resp = requests.get(url, headers=HEADERS)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            props = data.get(\"properties\", {})\n",
    "\n",
    "            maint_config = props.get(\"defaultTableMaintenanceConfiguration\", {})\n",
    "            if maint_config:\n",
    "                maintenance_info[\"maintenance_enabled\"] = \"ON\" if maint_config.get(\"enabled\", False) else \"OFF\"\n",
    "\n",
    "                vorder = maint_config.get(\"vOrderOptimization\", {})\n",
    "                maintenance_info[\"vorder_enabled\"] = \"ON\" if vorder.get(\"enabled\", False) else \"OFF\"\n",
    "\n",
    "                retention = maint_config.get(\"unreferencedFileRetention\", {})\n",
    "                if retention.get(\"enabled\", False):\n",
    "                    period = retention.get(\"retentionPeriod\", \"P7D\")\n",
    "                    days = parse_iso_duration_days(period)\n",
    "                    maintenance_info[\"retention_period_days\"] = days\n",
    "\n",
    "                compaction = maint_config.get(\"fileCompaction\", {})\n",
    "                maintenance_info[\"compaction_enabled\"] = \"ON\" if compaction.get(\"enabled\", False) else \"OFF\"\n",
    "            else:\n",
    "                maintenance_info[\"maintenance_enabled\"] = \"OFF\"\n",
    "\n",
    "        elif item_type == \"Warehouse\":\n",
    "            url = (f\"{FABRIC_API_BASE}/workspaces/{workspace_id}\"\n",
    "                   f\"/warehouses/{item_id}\")\n",
    "            resp = requests.get(url, headers=HEADERS)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # Warehouse auto-maintenance is always on (built-in)\n",
    "            maintenance_info[\"maintenance_enabled\"] = \"ON (Auto)\"\n",
    "            maintenance_info[\"vorder_enabled\"]      = \"ON (Auto)\"\n",
    "            maintenance_info[\"compaction_enabled\"]   = \"ON (Auto)\"\n",
    "            maintenance_info[\"retention_period_days\"] = None  # Managed by Fabric\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            maintenance_info[\"maintenance_enabled\"] = \"Not Available\"\n",
    "        else:\n",
    "            maintenance_info[\"maintenance_enabled\"] = f\"Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        maintenance_info[\"maintenance_enabled\"] = f\"Error: {str(e)[:50]}\"\n",
    "\n",
    "    return maintenance_info\n",
    "\n",
    "\n",
    "def parse_iso_duration_days(duration_str):\n",
    "    \"\"\"Parse ISO 8601 duration string (e.g. 'P7D', 'P30D') to days.\"\"\"\n",
    "    if not duration_str:\n",
    "        return None\n",
    "    match = re.match(r\"P(?:(\\d+)D)?\", duration_str)\n",
    "    if match and match.group(1):\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "# â”€â”€ Collect maintenance settings for all items â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "maintenance_data = []\n",
    "\n",
    "for item in all_items:\n",
    "    print(f\"ğŸ” Checking maintenance for {item['item_type']}: {item['item_name']} ...\")\n",
    "    settings = get_maintenance_settings(WORKSPACE_ID, item[\"item_id\"], item[\"item_type\"])\n",
    "    maintenance_data.append({\n",
    "        \"item_id\":               item[\"item_id\"],\n",
    "        \"item_name\":             item[\"item_name\"],\n",
    "        \"item_type\":             item[\"item_type\"],\n",
    "        \"maintenance_enabled\":   settings[\"maintenance_enabled\"],\n",
    "        \"retention_period_days\": settings[\"retention_period_days\"],\n",
    "        \"vorder_enabled\":        settings[\"vorder_enabled\"],\n",
    "        \"compaction_enabled\":    settings[\"compaction_enabled\"],\n",
    "    })\n",
    "    print(f\"   Maintenance: {settings['maintenance_enabled']}\")\n",
    "    if settings[\"retention_period_days\"]:\n",
    "        print(f\"   Retention:   {settings['retention_period_days']} days\")\n",
    "\n",
    "df_maintenance = pd.DataFrame(maintenance_data)\n",
    "\n",
    "print(f\"\\nâœ… Maintenance settings collected for {len(maintenance_data)} item(s)\")\n",
    "if not df_maintenance.empty:\n",
    "    print()\n",
    "    print(df_maintenance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9044543",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Consolidated Metrics Report\n",
    "\n",
    "Merges table-level metrics with item-level maintenance settings into a single report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d28338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Build Consolidated Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def format_size(size_kb):\n",
    "    \"\"\"Convert KB to human-readable size string.\"\"\"\n",
    "    if pd.isna(size_kb) or size_kb == 0:\n",
    "        return \"0 KB\"\n",
    "    if size_kb < 1024:\n",
    "        return f\"{size_kb:,.0f} KB\"\n",
    "    elif size_kb < 1024 * 1024:\n",
    "        return f\"{size_kb / 1024:,.2f} MB\"\n",
    "    else:\n",
    "        return f\"{size_kb / (1024 * 1024):,.2f} GB\"\n",
    "\n",
    "\n",
    "if not df_metrics.empty and not df_maintenance.empty:\n",
    "    # Merge table metrics with maintenance settings\n",
    "    df_report = df_metrics.merge(\n",
    "        df_maintenance[[\"item_id\", \"maintenance_enabled\", \"retention_period_days\",\n",
    "                        \"vorder_enabled\", \"compaction_enabled\"]],\n",
    "        on=\"item_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Format size columns for display\n",
    "    df_report[\"reserved_size_display\"] = df_report[\"reserved_size_kb\"].apply(format_size)\n",
    "    df_report[\"used_size_display\"]     = df_report[\"used_size_kb\"].apply(format_size)\n",
    "\n",
    "    # Format retention days\n",
    "    df_report[\"retention_days_display\"] = df_report[\"retention_period_days\"].apply(\n",
    "        lambda x: f\"{int(x)} days\" if pd.notna(x) else \"N/A (Auto-managed)\"\n",
    "    )\n",
    "\n",
    "    # Select and rename columns for display\n",
    "    df_display = df_report[[\n",
    "        \"item_type\", \"item_name\", \"schema_name\", \"table_name\",\n",
    "        \"row_count\", \"reserved_size_display\", \"used_size_display\",\n",
    "        \"maintenance_enabled\", \"retention_days_display\",\n",
    "        \"vorder_enabled\", \"compaction_enabled\"\n",
    "    ]].rename(columns={\n",
    "        \"item_type\":              \"Type\",\n",
    "        \"item_name\":              \"Warehouse / Lakehouse\",\n",
    "        \"schema_name\":            \"Schema\",\n",
    "        \"table_name\":             \"Table Name\",\n",
    "        \"row_count\":              \"Row Count\",\n",
    "        \"reserved_size_display\":  \"Size on Disk\",\n",
    "        \"used_size_display\":      \"Used Size\",\n",
    "        \"maintenance_enabled\":    \"Maintenance\",\n",
    "        \"retention_days_display\": \"Retention\",\n",
    "        \"vorder_enabled\":         \"V-Order\",\n",
    "        \"compaction_enabled\":     \"Compaction\",\n",
    "    })\n",
    "\n",
    "    print(\"ğŸ“‹ FABRIC WAREHOUSE & LAKEHOUSE METRICS REPORT\")\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"   Workspace: {ws['displayName']}\")\n",
    "    print(f\"   Items:     {len(all_items)} ({len(warehouses)} Warehouse, {len(lakehouses)} Lakehouse)\")\n",
    "    print(f\"   Tables:    {len(df_display)}\")\n",
    "    total_rows = df_report['row_count'].sum()\n",
    "    total_size = df_report['reserved_size_kb'].sum()\n",
    "    print(f\"   Total Rows: {total_rows:,.0f}\")\n",
    "    print(f\"   Total Size: {format_size(total_size)}\")\n",
    "    print(\"=\" * 120)\n",
    "    print()\n",
    "    print(df_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  No data to display. Ensure previous cells ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5596d4",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Summary by Warehouse / Lakehouse\n",
    "\n",
    "Aggregated view showing totals per item with maintenance status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e531f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Summary by Item â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_metrics.empty and not df_maintenance.empty:\n",
    "    df_summary = df_report.groupby([\"item_type\", \"item_name\", \"maintenance_enabled\",\n",
    "                                     \"retention_days_display\"]).agg(\n",
    "        table_count   = (\"table_name\", \"count\"),\n",
    "        total_rows    = (\"row_count\", \"sum\"),\n",
    "        total_size_kb = (\"reserved_size_kb\", \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    df_summary[\"total_size\"] = df_summary[\"total_size_kb\"].apply(format_size)\n",
    "\n",
    "    df_summary_display = df_summary[[\n",
    "        \"item_type\", \"item_name\", \"table_count\", \"total_rows\",\n",
    "        \"total_size\", \"maintenance_enabled\", \"retention_days_display\"\n",
    "    ]].rename(columns={\n",
    "        \"item_type\":              \"Type\",\n",
    "        \"item_name\":              \"Name\",\n",
    "        \"table_count\":            \"Tables\",\n",
    "        \"total_rows\":             \"Total Rows\",\n",
    "        \"total_size\":             \"Total Size\",\n",
    "        \"maintenance_enabled\":    \"Maintenance\",\n",
    "        \"retention_days_display\": \"Retention\",\n",
    "    })\n",
    "\n",
    "    print(\"ğŸ“Š SUMMARY BY WAREHOUSE / LAKEHOUSE\")\n",
    "    print(\"=\" * 100)\n",
    "    print()\n",
    "    print(df_summary_display.to_string(index=False))\n",
    "\n",
    "    # â”€â”€ Maintenance Status Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\")\n",
    "    print(\"ğŸ”§ MAINTENANCE STATUS\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in df_summary.iterrows():\n",
    "        icon = \"âœ…\" if \"ON\" in str(row[\"maintenance_enabled\"]) else \"âŒ\"\n",
    "        retention = row[\"retention_days_display\"]\n",
    "        print(f\"   {icon} {row['item_name']} ({row['item_type']})\")\n",
    "        print(f\"      Maintenance: {row['maintenance_enabled']}\")\n",
    "        print(f\"      Retention:   {retention}\")\n",
    "        print(f\"      Tables: {row['table_count']:,}  |  Rows: {row['total_rows']:,.0f}  |  Size: {row['total_size']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸  No data to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b131193",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Export Report to CSV\n",
    "\n",
    "Saves the full metrics report to a CSV file for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export to CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if not df_metrics.empty:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"fabric_metrics_{timestamp}.csv\"\n",
    "\n",
    "    export_df = df_report[[\n",
    "        \"item_type\", \"item_name\", \"schema_name\", \"table_name\",\n",
    "        \"row_count\", \"reserved_size_kb\", \"used_size_kb\",\n",
    "        \"created_date\", \"modified_date\",\n",
    "        \"maintenance_enabled\", \"retention_period_days\",\n",
    "        \"vorder_enabled\", \"compaction_enabled\"\n",
    "    ]].copy()\n",
    "\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Report exported to: {output_file}\")\n",
    "    print(f\"   Rows: {len(export_df)}\")\n",
    "    print(f\"   Columns: {', '.join(export_df.columns)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d704c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Notes\n",
    "\n",
    "### Running This Notebook\n",
    "- **Import** into any Fabric workspace via the deployment notebook or Fabric portal\n",
    "- **Attach a Lakehouse** (optional) to enable CSV export and Delta table persistence\n",
    "- Set `TARGET_WORKSPACE_ID` in Cell 5 to scan a different workspace, or leave `None` for the current one\n",
    "\n",
    "### Table Maintenance in Microsoft Fabric\n",
    "\n",
    "| Feature | Warehouse | Lakehouse |\n",
    "|---------|-----------|----------|\n",
    "| **Auto-Statistics** | Always ON (built-in) | N/A (Delta-based) |\n",
    "| **V-Order Optimization** | Always ON (auto) | Configurable (ON/OFF) |\n",
    "| **File Compaction** | Always ON (auto) | Configurable (ON/OFF) |\n",
    "| **Unreferenced File Removal** | Auto-managed | Configurable with retention days |\n",
    "| **Maintenance Schedule** | Automatic | User-configurable |\n",
    "\n",
    "### Maintenance Retention Days (Lakehouse)\n",
    "- Default: **7 days** for unreferenced file retention\n",
    "- Configurable from **1 to 90 days** via Fabric portal or REST API\n",
    "- Lower values â†’ faster cleanup, less time-travel capability\n",
    "- Higher values â†’ more storage cost, longer rollback window\n",
    "\n",
    "### SQL Endpoint DMVs Used\n",
    "- `sys.tables` â€“ table metadata\n",
    "- `sys.schemas` â€“ schema names\n",
    "- `sys.dm_db_partition_stats` â€“ row counts and page-level size metrics\n",
    "\n",
    "### Fabric-Native Libraries Used\n",
    "- `sempy.fabric` â€“ workspace context and item discovery\n",
    "- `mssparkutils.credentials` â€“ automatic Entra ID token acquisition\n",
    "- `spark.read.format(\"jdbc\")` â€“ Spark JDBC connector for SQL endpoint queries\n",
    "- `notebookutils.mssparkutils` â€“ file system operations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}