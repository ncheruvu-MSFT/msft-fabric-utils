{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392b233d",
   "metadata": {},
   "source": [
    "# ğŸš€ Microsoft Fabric â€“ Warehouse Performance & OneLake Soft Delete Monitor\n",
    "\n",
    "This notebook provides two key capabilities for Microsoft Fabric governance:\n",
    "\n",
    "**Part A â€“ Warehouse Performance Diagnostics** based on [Performance guidelines in Fabric Data Warehouse](https://learn.microsoft.com/en-us/fabric/data-warehouse/guidelines-warehouse-performance):\n",
    "\n",
    "| Check | Source |\n",
    "|-------|--------|\n",
    "| Cold Cache Detection | `queryinsights.exec_requests_history` â€“ `data_scanned_remote_storage_mb` |\n",
    "| Long-Running Queries | `queryinsights.long_running_queries` |\n",
    "| Frequently Run Queries | `queryinsights.frequently_run_queries` |\n",
    "| Statistics Freshness | `sys.stats` + `DBCC SHOW_STATISTICS` |\n",
    "| String Column Sizing | `sys.columns` â€“ oversized `varchar(8000)` / `varchar(max)` detection |\n",
    "| Data Type Optimization | `sys.columns` â€“ nullable columns, `decimal(18,0)` overuse |\n",
    "| Active Locks & Blocking | `sys.dm_tran_locks` |\n",
    "| Table Compaction Health | `sys.dm_db_partition_stats` â€“ small/fragmented row groups |\n",
    "| V-Order Status | Fabric REST API â€“ warehouse properties |\n",
    "\n",
    "**Part B â€“ OneLake Soft Delete Scanner** based on [OneLake Disaster Recovery â€“ Soft Delete](https://learn.microsoft.com/en-us/fabric/onelake/onelake-disaster-recovery#soft-delete-for-onelake-files):\n",
    "\n",
    "| Check | Source |\n",
    "|-------|--------|\n",
    "| Soft-Deleted Files | OneLake DFS API via `azure-storage-file-datalake` |\n",
    "| Remaining Recovery Days | Blob metadata â€“ `RemainingDaysBeforePermanentDelete` |\n",
    "| Storage Cost of Deleted Data | Calculated from soft-deleted file sizes |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"ğŸ” Fabric Auth\"] --> B[\"ğŸ“¡ Fabric REST API\"]\n",
    "    A --> C[\"ğŸ—„ï¸ SQL Endpoint\"]\n",
    "    A --> D[\"ğŸ“‚ OneLake DFS\"]\n",
    "    B --> E[\"ğŸ“¦ Discover Items\"]\n",
    "    C --> F[\"ğŸ“Š Query Insights\"]\n",
    "    C --> G[\"ğŸ” Schema Analysis\"]\n",
    "    D --> H[\"ğŸ—‘ï¸ Soft Delete Scan\"]\n",
    "    F --> I[\"ğŸ“‹ Performance Report\"]\n",
    "    G --> I\n",
    "    H --> J[\"ğŸ“‹ Soft Delete Report\"]\n",
    "    I --> K[\"ğŸ’¾ CSV Export\"]\n",
    "    J --> K\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- This notebook runs **inside Microsoft Fabric** (not externally)\n",
    "- Fabric workspace Admin or Member role\n",
    "- Pre-installed libraries: `sempy`, `mssparkutils`, `pandas`, `requests`, `pyspark`\n",
    "- For Soft Delete scanning: `azure-storage-file-datalake` (install cell provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e7b5a",
   "metadata": {},
   "source": [
    "## 0ï¸âƒ£ Verify Fabric Environment & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a08688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify Fabric Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "from notebookutils import mssparkutils\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "print(\"âœ… Fabric environment verified\")\n",
    "print(f\"   sempy version:      {fabric.__version__ if hasattr(fabric, '__version__') else 'OK'}\")\n",
    "print(f\"   mssparkutils:       available\")\n",
    "print(f\"   pandas version:     {pd.__version__}\")\n",
    "print(f\"   spark:              {spark.version}\")\n",
    "\n",
    "# Install azure-storage-file-datalake for soft-delete scanning\n",
    "try:\n",
    "    from azure.storage.filedatalake import DataLakeServiceClient\n",
    "    print(\"   azure-storage-file-datalake: already installed\")\n",
    "except ImportError:\n",
    "    print(\"   Installing azure-storage-file-datalake ...\")\n",
    "    %pip install -q azure-storage-file-datalake\n",
    "    from azure.storage.filedatalake import DataLakeServiceClient\n",
    "    print(\"   azure-storage-file-datalake: installed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daeed60",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9281cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ Fabric Workspace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to None to use the current workspace, or specify a workspace GUID\n",
    "TARGET_WORKSPACE_ID = None\n",
    "\n",
    "# â”€â”€ Feature Toggles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to True/False to enable/disable each section\n",
    "RUN_PERFORMANCE_DIAGNOSTICS = True   # Part A: Warehouse performance checks\n",
    "RUN_SOFT_DELETE_SCAN        = True   # Part B: OneLake soft delete scan\n",
    "\n",
    "# â”€â”€ Performance Diagnostics Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Thresholds for performance alerts\n",
    "COLD_CACHE_THRESHOLD_MB     = 100    # Flag queries reading > N MB from remote storage\n",
    "LONG_QUERY_THRESHOLD_SEC    = 300    # Flag queries running longer than N seconds\n",
    "VARCHAR_OVERSIZED_THRESHOLD = 4000   # Flag varchar(n) columns where n > this value\n",
    "TOP_N_QUERIES               = 20     # Number of top queries to show in reports\n",
    "\n",
    "# â”€â”€ Soft Delete Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SCAN_ITEM_TYPES = (\"Lakehouse\", \"Warehouse\")  # Item types to scan for soft deletes\n",
    "\n",
    "# â”€â”€ Resolve workspace ID â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if TARGET_WORKSPACE_ID:\n",
    "    WORKSPACE_ID = TARGET_WORKSPACE_ID\n",
    "else:\n",
    "    WORKSPACE_ID = fabric.get_workspace_id()\n",
    "\n",
    "# â”€â”€ Fabric API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "# â”€â”€ Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fabric_token = mssparkutils.credentials.getToken(\"pbi\")\n",
    "sql_token    = mssparkutils.credentials.getToken(\"https://database.windows.net\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {fabric_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# â”€â”€ Display Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# â”€â”€ Validate workspace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "resp = requests.get(f\"{FABRIC_API_BASE}/workspaces/{WORKSPACE_ID}\", headers=HEADERS)\n",
    "resp.raise_for_status()\n",
    "ws = resp.json()\n",
    "print(f\"ğŸš€ Warehouse Performance & Soft Delete Monitor\")\n",
    "print(f\"   Workspace:      {ws['displayName']}\")\n",
    "print(f\"   Workspace ID:   {WORKSPACE_ID}\")\n",
    "print(f\"   Capacity:       {ws.get('capacityId', 'N/A')}\")\n",
    "print(f\"   Performance:    {'âœ… Enabled' if RUN_PERFORMANCE_DIAGNOSTICS else 'â­ï¸ Skipped'}\")\n",
    "print(f\"   Soft Delete:    {'âœ… Enabled' if RUN_SOFT_DELETE_SCAN else 'â­ï¸ Skipped'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593b8bd",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Discover Warehouses & Lakehouses\n",
    "\n",
    "Identifies all Warehouses and Lakehouses in the workspace for both performance analysis and soft-delete scanning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a15683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Discover Warehouses & Lakehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def list_fabric_items(workspace_id, item_type, headers):\n",
    "    \"\"\"List all items of a given type in a Fabric workspace (handles pagination).\"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/workspaces/{workspace_id}/{item_type}\"\n",
    "    items = []\n",
    "    while url:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items.extend(data.get(\"value\", []))\n",
    "        url = data.get(\"continuationUri\")\n",
    "    return items\n",
    "\n",
    "# â”€â”€ Warehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "raw_warehouses = list_fabric_items(WORKSPACE_ID, \"warehouses\", HEADERS)\n",
    "warehouses = []\n",
    "for w in raw_warehouses:\n",
    "    props = w.get(\"properties\", {})\n",
    "    conn_string = props.get(\"connectionString\", \"\")\n",
    "    sql_ep = conn_string.replace(\"jdbc:sqlserver://\", \"\").split(\";\")[0] if conn_string else \"\"\n",
    "    warehouses.append({\n",
    "        \"item_id\":       w[\"id\"],\n",
    "        \"item_name\":     w[\"displayName\"],\n",
    "        \"item_type\":     \"Warehouse\",\n",
    "        \"sql_endpoint\":  sql_ep,\n",
    "        \"database_name\": w[\"displayName\"],\n",
    "    })\n",
    "print(f\"ğŸ“¦ Found {len(warehouses)} Warehouse(s)\")\n",
    "\n",
    "# â”€â”€ Lakehouses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "raw_lakehouses = list_fabric_items(WORKSPACE_ID, \"lakehouses\", HEADERS)\n",
    "lakehouses = []\n",
    "for lh in raw_lakehouses:\n",
    "    props = lh.get(\"properties\", {})\n",
    "    sql_ep = props.get(\"sqlEndpointProperties\", {}).get(\"connectionString\", \"\")\n",
    "    lakehouses.append({\n",
    "        \"item_id\":       lh[\"id\"],\n",
    "        \"item_name\":     lh[\"displayName\"],\n",
    "        \"item_type\":     \"Lakehouse\",\n",
    "        \"sql_endpoint\":  sql_ep,\n",
    "        \"database_name\": lh[\"displayName\"],\n",
    "    })\n",
    "print(f\"ğŸ  Found {len(lakehouses)} Lakehouse(s)\")\n",
    "\n",
    "# â”€â”€ Combined list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_items = warehouses + lakehouses\n",
    "print(f\"\\nâœ… Total: {len(all_items)} item(s)\")\n",
    "for item in all_items:\n",
    "    print(f\"   {item['item_type']:10s} | {item['item_name']} | SQL: {item['sql_endpoint'][:60] if item['sql_endpoint'] else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886445a1",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Warehouse Performance Diagnostics\n",
    "\n",
    "Performance checks based on [Microsoft Fabric Warehouse Performance Guidelines](https://learn.microsoft.com/en-us/fabric/data-warehouse/guidelines-warehouse-performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072903a",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ SQL Endpoint Query Helper\n",
    "\n",
    "Reusable function to query Warehouse/Lakehouse SQL endpoints via Spark JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9051ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SQL Endpoint Query Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def query_sql_endpoint(sql_endpoint, database_name, access_token, query):\n",
    "    \"\"\"Query a Fabric SQL endpoint using Spark JDBC. Returns a pandas DataFrame.\"\"\"\n",
    "    jdbc_url = (\n",
    "        f\"jdbc:sqlserver://{sql_endpoint};\"\n",
    "        f\"database={database_name};\"\n",
    "        f\"encrypt=true;\"\n",
    "        f\"trustServerCertificate=false;\"\n",
    "        f\"loginTimeout=30;\"\n",
    "    )\n",
    "    spark_df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", query)\n",
    "        .option(\"accessToken\", access_token)\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "        .load()\n",
    "    )\n",
    "    try:\n",
    "        return spark_df.toPandas()\n",
    "    except (TypeError, ValueError):\n",
    "        from pyspark.sql.functions import col\n",
    "        for field in spark_df.schema.fields:\n",
    "            if \"date\" in field.dataType.simpleString() or \"timestamp\" in field.dataType.simpleString():\n",
    "                spark_df = spark_df.withColumn(field.name, col(field.name).cast(\"string\"))\n",
    "        return spark_df.toPandas()\n",
    "\n",
    "\n",
    "def safe_query(sql_endpoint, database_name, access_token, query, label=\"query\"):\n",
    "    \"\"\"Execute a query with error handling. Returns DataFrame or None.\"\"\"\n",
    "    try:\n",
    "        return query_sql_endpoint(sql_endpoint, database_name, access_token, query)\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸  {label} failed: {str(e)[:120]}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926f77e",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Cold Cache & Remote Storage Detection\n",
    "\n",
    "Queries `queryinsights.exec_requests_history` to detect queries that fetched data from remote storage (OneLake) instead of cache.\n",
    "A non-zero `data_scanned_remote_storage_mb` indicates a **cold start** â€” the query had to load data from OneLake into memory.\n",
    "\n",
    "> **Best Practice:** Don't judge query performance based on the first execution. Subsequent runs use cached data and are significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a89032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cold Cache & Remote Storage Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "COLD_CACHE_QUERY = f\"\"\"(\n",
    "SELECT TOP {TOP_N_QUERIES}\n",
    "    CONVERT(VARCHAR(30), start_time, 126)        AS start_time,\n",
    "    CONVERT(VARCHAR(30), end_time, 126)          AS end_time,\n",
    "    DATEDIFF(SECOND, start_time, end_time)       AS duration_sec,\n",
    "    data_scanned_remote_storage_mb,\n",
    "    data_scanned_memory_mb,\n",
    "    CAST(command AS VARCHAR(500))                 AS query_text,\n",
    "    status,\n",
    "    session_id,\n",
    "    distributed_statement_id\n",
    "FROM queryinsights.exec_requests_history\n",
    "WHERE data_scanned_remote_storage_mb > {COLD_CACHE_THRESHOLD_MB}\n",
    "ORDER BY data_scanned_remote_storage_mb DESC\n",
    ") AS cold_cache\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_cold_cache = []\n",
    "\n",
    "    for item in warehouses:\n",
    "        sql_ep    = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"â„ï¸  Checking cold cache for: {item_name} ...\")\n",
    "        df = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                        COLD_CACHE_QUERY, \"Cold cache query\")\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"warehouse_name\"] = item_name\n",
    "            all_cold_cache.append(df)\n",
    "            print(f\"   Found {len(df)} queries with significant remote storage reads\")\n",
    "        else:\n",
    "            print(f\"   âœ… No significant cold cache hits detected\")\n",
    "\n",
    "    if all_cold_cache:\n",
    "        df_cold_cache = pd.concat(all_cold_cache, ignore_index=True)\n",
    "        print(f\"\\nâ„ï¸  COLD CACHE REPORT\")\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"   Queries fetching > {COLD_CACHE_THRESHOLD_MB} MB from remote storage: {len(df_cold_cache)}\")\n",
    "        print()\n",
    "        display_cols = [\"warehouse_name\", \"start_time\", \"duration_sec\",\n",
    "                        \"data_scanned_remote_storage_mb\", \"data_scanned_memory_mb\", \"query_text\"]\n",
    "        print(df_cold_cache[[c for c in display_cols if c in df_cold_cache.columns]].to_string(index=False))\n",
    "        print(\"\\nğŸ’¡ Tip: Subsequent executions of these queries should be faster once data is cached.\")\n",
    "    else:\n",
    "        df_cold_cache = pd.DataFrame()\n",
    "        print(\"\\nâœ… No significant cold cache issues detected across all warehouses.\")\n",
    "else:\n",
    "    print(\"â­ï¸  Performance diagnostics skipped (RUN_PERFORMANCE_DIAGNOSTICS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9101ea",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Long-Running & Frequently Run Queries\n",
    "\n",
    "Uses the built-in `queryinsights` views to identify:\n",
    "- **Long-running queries** â€“ queries that take an unusually long time to execute\n",
    "- **Frequently run queries** â€“ queries executed most often (candidates for optimization or caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6565f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Long-Running & Frequently Run Queries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "LONG_RUNNING_QUERY = f\"\"\"(\n",
    "SELECT TOP {TOP_N_QUERIES}\n",
    "    CAST(query_hash AS VARCHAR(100))             AS query_hash,\n",
    "    CAST(query_text AS VARCHAR(500))             AS query_text,\n",
    "    execution_count,\n",
    "    median_duration_ms,\n",
    "    median_duration_ms / 1000.0                  AS median_duration_sec,\n",
    "    max_duration_ms,\n",
    "    max_duration_ms / 1000.0                     AS max_duration_sec,\n",
    "    median_row_count,\n",
    "    CONVERT(VARCHAR(30), last_execution_start_time, 126) AS last_execution\n",
    "FROM queryinsights.long_running_queries\n",
    "ORDER BY median_duration_ms DESC\n",
    ") AS long_running\"\"\"\n",
    "\n",
    "FREQUENTLY_RUN_QUERY = f\"\"\"(\n",
    "SELECT TOP {TOP_N_QUERIES}\n",
    "    CAST(query_hash AS VARCHAR(100))             AS query_hash,\n",
    "    CAST(query_text AS VARCHAR(500))             AS query_text,\n",
    "    execution_count,\n",
    "    median_duration_ms,\n",
    "    median_duration_ms / 1000.0                  AS median_duration_sec,\n",
    "    median_row_count,\n",
    "    CONVERT(VARCHAR(30), last_execution_start_time, 126) AS last_execution\n",
    "FROM queryinsights.frequently_run_queries\n",
    "ORDER BY execution_count DESC\n",
    ") AS freq_run\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_long_running = []\n",
    "    all_frequent = []\n",
    "\n",
    "    for item in warehouses:\n",
    "        sql_ep    = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ¢ Checking long-running queries for: {item_name} ...\")\n",
    "        df_lr = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                           LONG_RUNNING_QUERY, \"Long-running queries\")\n",
    "        if df_lr is not None and not df_lr.empty:\n",
    "            df_lr[\"warehouse_name\"] = item_name\n",
    "            all_long_running.append(df_lr)\n",
    "            print(f\"   Found {len(df_lr)} long-running query pattern(s)\")\n",
    "\n",
    "        print(f\"ğŸ”„ Checking frequently run queries for: {item_name} ...\")\n",
    "        df_fr = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                           FREQUENTLY_RUN_QUERY, \"Frequent queries\")\n",
    "        if df_fr is not None and not df_fr.empty:\n",
    "            df_fr[\"warehouse_name\"] = item_name\n",
    "            all_frequent.append(df_fr)\n",
    "            print(f\"   Found {len(df_fr)} frequently executed query pattern(s)\")\n",
    "\n",
    "    # â”€â”€ Long-Running Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if all_long_running:\n",
    "        df_long_running = pd.concat(all_long_running, ignore_index=True)\n",
    "        print(f\"\\nğŸ¢ LONG-RUNNING QUERIES\")\n",
    "        print(\"=\" * 130)\n",
    "        for _, row in df_long_running.iterrows():\n",
    "            dur = row.get(\"median_duration_sec\", 0) or 0\n",
    "            icon = \"ğŸ”´\" if dur > LONG_QUERY_THRESHOLD_SEC else \"ğŸŸ¡\"\n",
    "            print(f\"   {icon} [{row.get('warehouse_name', '')}] \"\n",
    "                  f\"Median: {dur:.1f}s | Max: {row.get('max_duration_sec', 0):.1f}s | \"\n",
    "                  f\"Runs: {row.get('execution_count', 0)} | \"\n",
    "                  f\"{str(row.get('query_text', ''))[:80]}\")\n",
    "    else:\n",
    "        df_long_running = pd.DataFrame()\n",
    "        print(\"\\nâœ… No long-running queries detected.\")\n",
    "\n",
    "    # â”€â”€ Frequently Run Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if all_frequent:\n",
    "        df_frequent = pd.concat(all_frequent, ignore_index=True)\n",
    "        print(f\"\\nğŸ”„ FREQUENTLY RUN QUERIES\")\n",
    "        print(\"=\" * 130)\n",
    "        for _, row in df_frequent.iterrows():\n",
    "            print(f\"   ğŸ“Š [{row.get('warehouse_name', '')}] \"\n",
    "                  f\"Executions: {row.get('execution_count', 0):,} | \"\n",
    "                  f\"Median: {row.get('median_duration_sec', 0):.1f}s | \"\n",
    "                  f\"{str(row.get('query_text', ''))[:80]}\")\n",
    "        print(f\"\\nğŸ’¡ Tip: Frequently run queries are prime candidates for optimization and caching.\")\n",
    "    else:\n",
    "        df_frequent = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37508a33",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Statistics Freshness Check\n",
    "\n",
    "Checks whether **auto-created statistics** are stale or missing. Fabric Warehouse automatically maintains histogram statistics, average column length statistics, and table cardinality statistics. However, if there's a large window between table transformations and query workloads, manually updating statistics during a maintenance window can improve query performance.\n",
    "\n",
    "> **Best Practice:** Use `CREATE STATISTICS` or `UPDATE STATISTICS` T-SQL commands during maintenance windows to reduce the likelihood of `SELECT` queries having to first update statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8698531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Statistics Freshness Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "STATS_QUERY = \"\"\"(\n",
    "SELECT\n",
    "    s.name                                       AS schema_name,\n",
    "    t.name                                       AS table_name,\n",
    "    st.name                                      AS stats_name,\n",
    "    st.auto_created,\n",
    "    st.user_created,\n",
    "    CONVERT(VARCHAR(30), sp.last_updated, 126)   AS last_updated,\n",
    "    sp.rows,\n",
    "    sp.rows_sampled,\n",
    "    sp.modification_counter\n",
    "FROM sys.stats st\n",
    "INNER JOIN sys.tables t\n",
    "    ON st.object_id = t.object_id\n",
    "INNER JOIN sys.schemas s\n",
    "    ON t.schema_id = s.schema_id\n",
    "CROSS APPLY sys.dm_db_stats_properties(st.object_id, st.stats_id) sp\n",
    "WHERE t.is_ms_shipped = 0\n",
    "ORDER BY sp.modification_counter DESC, sp.last_updated ASC\n",
    ") AS stats_info\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_stats = []\n",
    "\n",
    "    for item in warehouses:\n",
    "        sql_ep = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ“Š Checking statistics freshness for: {item_name} ...\")\n",
    "        df = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                        STATS_QUERY, \"Statistics freshness\")\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"warehouse_name\"] = item_name\n",
    "            all_stats.append(df)\n",
    "\n",
    "            # Flag stale stats (high modification counter relative to rows)\n",
    "            stale = df[df[\"modification_counter\"] > 0].copy()\n",
    "            if not stale.empty:\n",
    "                stale[\"stale_pct\"] = (stale[\"modification_counter\"] / stale[\"rows\"].clip(lower=1) * 100)\n",
    "                very_stale = stale[stale[\"stale_pct\"] > 20]\n",
    "                if not very_stale.empty:\n",
    "                    print(f\"   âš ï¸  {len(very_stale)} statistics are >20% stale (rows modified since last stats update)\")\n",
    "                    for _, row in very_stale.head(5).iterrows():\n",
    "                        print(f\"      - {row['schema_name']}.{row['table_name']}.{row['stats_name']}: \"\n",
    "                              f\"{row['modification_counter']:,} modifications ({row['stale_pct']:.0f}% of rows)\")\n",
    "                else:\n",
    "                    print(f\"   âœ… All statistics are reasonably fresh\")\n",
    "            else:\n",
    "                print(f\"   âœ… No modified rows since last statistics update\")\n",
    "\n",
    "    if all_stats:\n",
    "        df_all_stats = pd.concat(all_stats, ignore_index=True)\n",
    "        print(f\"\\nğŸ“Š STATISTICS FRESHNESS SUMMARY\")\n",
    "        print(\"=\" * 100)\n",
    "        total_stats = len(df_all_stats)\n",
    "        stale_stats = len(df_all_stats[df_all_stats[\"modification_counter\"] > 0])\n",
    "        print(f\"   Total statistics objects: {total_stats}\")\n",
    "        print(f\"   With modifications:       {stale_stats}\")\n",
    "        print(f\"\\nğŸ’¡ Tip: Run 'UPDATE STATISTICS <table>' during maintenance windows for stale statistics.\")\n",
    "    else:\n",
    "        df_all_stats = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb8079",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Schema & Data Type Analysis\n",
    "\n",
    "Identifies columns that may hurt performance based on the warehouse performance guidelines:\n",
    "\n",
    "| Issue | Impact | Recommendation |\n",
    "|-------|--------|----------------|\n",
    "| `varchar(8000)` / `varchar(max)` | Inaccurate statistics, suboptimal query plans | Use specific `varchar(n)` lengths |\n",
    "| Oversized `decimal(18,0)` | Wastes 9 bytes/row when `int` (4 bytes) suffices | Use `int`/`bigint` for whole numbers |\n",
    "| Nullable columns | Metadata overhead, reduced optimization | Use `NOT NULL` when possible |\n",
    "| `char(n)` for variable-length data | Wasted space, I/O overhead | Use `varchar(n)` unless fixed-length |\n",
    "| Data type mismatches in JOINs | Implicit conversions, poor performance | Ensure JOIN columns have matching types |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Schema & Data Type Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SCHEMA_ANALYSIS_QUERY = f\"\"\"(\n",
    "SELECT\n",
    "    s.name                  AS schema_name,\n",
    "    t.name                  AS table_name,\n",
    "    c.name                  AS column_name,\n",
    "    ty.name                 AS data_type,\n",
    "    c.max_length,\n",
    "    c.precision,\n",
    "    c.scale,\n",
    "    c.is_nullable,\n",
    "    c.column_id\n",
    "FROM sys.columns c\n",
    "INNER JOIN sys.tables t  ON c.object_id = t.object_id\n",
    "INNER JOIN sys.schemas s ON t.schema_id = s.schema_id\n",
    "INNER JOIN sys.types ty  ON c.user_type_id = ty.user_type_id\n",
    "WHERE t.is_ms_shipped = 0\n",
    "ORDER BY s.name, t.name, c.column_id\n",
    ") AS schema_info\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_schema_issues = []\n",
    "    all_schema_data = []\n",
    "\n",
    "    for item in warehouses + lakehouses:\n",
    "        sql_ep = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "        item_type = item[\"item_type\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ” Analyzing schema for {item_type}: {item_name} ...\")\n",
    "        df = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                        SCHEMA_ANALYSIS_QUERY, \"Schema analysis\")\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"item_name\"] = item_name\n",
    "        df[\"item_type\"] = item_type\n",
    "        all_schema_data.append(df)\n",
    "\n",
    "        issues = []\n",
    "\n",
    "        # Check 1: Oversized varchar columns\n",
    "        oversized_varchar = df[\n",
    "            (df[\"data_type\"].isin([\"varchar\", \"nvarchar\"])) &\n",
    "            (df[\"max_length\"] > VARCHAR_OVERSIZED_THRESHOLD)\n",
    "        ]\n",
    "        if not oversized_varchar.empty:\n",
    "            for _, row in oversized_varchar.iterrows():\n",
    "                length_display = \"max\" if row[\"max_length\"] == -1 else str(row[\"max_length\"])\n",
    "                issues.append({\n",
    "                    \"item_name\": item_name,\n",
    "                    \"item_type\": item_type,\n",
    "                    \"table\": f\"{row['schema_name']}.{row['table_name']}\",\n",
    "                    \"column\": row[\"column_name\"],\n",
    "                    \"issue\": f\"Oversized string: {row['data_type']}({length_display})\",\n",
    "                    \"severity\": \"ğŸ”´ High\",\n",
    "                    \"recommendation\": f\"Use {row['data_type']}(n) with appropriate max length\"\n",
    "                })\n",
    "\n",
    "        # Check 2: decimal(18,0) â€“ could often be int/bigint\n",
    "        inefficient_decimal = df[\n",
    "            (df[\"data_type\"].isin([\"decimal\", \"numeric\"])) &\n",
    "            (df[\"precision\"] == 18) & (df[\"scale\"] == 0)\n",
    "        ]\n",
    "        if not inefficient_decimal.empty:\n",
    "            for _, row in inefficient_decimal.iterrows():\n",
    "                issues.append({\n",
    "                    \"item_name\": item_name,\n",
    "                    \"item_type\": item_type,\n",
    "                    \"table\": f\"{row['schema_name']}.{row['table_name']}\",\n",
    "                    \"column\": row[\"column_name\"],\n",
    "                    \"issue\": \"decimal(18,0) â€“ 9 bytes/row\",\n",
    "                    \"severity\": \"ğŸŸ¡ Medium\",\n",
    "                    \"recommendation\": \"Consider int (4 bytes) or bigint (8 bytes) for whole numbers\"\n",
    "                })\n",
    "\n",
    "        # Check 3: Nullable columns that could be NOT NULL\n",
    "        nullable_cols = df[df[\"is_nullable\"] == True]\n",
    "        nullable_pct = len(nullable_cols) / len(df) * 100 if len(df) > 0 else 0\n",
    "        if nullable_pct > 80:\n",
    "            issues.append({\n",
    "                \"item_name\": item_name,\n",
    "                \"item_type\": item_type,\n",
    "                \"table\": \"(all tables)\",\n",
    "                \"column\": \"(multiple)\",\n",
    "                \"issue\": f\"{nullable_pct:.0f}% of columns are nullable\",\n",
    "                \"severity\": \"ğŸŸ¡ Medium\",\n",
    "                \"recommendation\": \"Define columns as NOT NULL when the data model allows\"\n",
    "            })\n",
    "\n",
    "        # Check 4: char vs varchar opportunity\n",
    "        char_cols = df[df[\"data_type\"].isin([\"char\", \"nchar\"])]\n",
    "        large_char = char_cols[char_cols[\"max_length\"] > 20]\n",
    "        if not large_char.empty:\n",
    "            for _, row in large_char.iterrows():\n",
    "                issues.append({\n",
    "                    \"item_name\": item_name,\n",
    "                    \"item_type\": item_type,\n",
    "                    \"table\": f\"{row['schema_name']}.{row['table_name']}\",\n",
    "                    \"column\": row[\"column_name\"],\n",
    "                    \"issue\": f\"Fixed-length char({row['max_length']})\",\n",
    "                    \"severity\": \"ğŸŸ¡ Medium\",\n",
    "                    \"recommendation\": \"Use varchar(n) unless data is truly fixed-length\"\n",
    "                })\n",
    "\n",
    "        all_schema_issues.extend(issues)\n",
    "\n",
    "        if issues:\n",
    "            print(f\"   âš ï¸  {len(issues)} schema optimization issue(s) found\")\n",
    "        else:\n",
    "            print(f\"   âœ… Schema looks well-optimized\")\n",
    "\n",
    "    # â”€â”€ Schema Issues Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if all_schema_issues:\n",
    "        df_schema_issues = pd.DataFrame(all_schema_issues)\n",
    "        print(f\"\\nğŸ” SCHEMA & DATA TYPE OPTIMIZATION REPORT\")\n",
    "        print(\"=\" * 130)\n",
    "        print(f\"   Total issues found: {len(df_schema_issues)}\")\n",
    "\n",
    "        high = len(df_schema_issues[df_schema_issues[\"severity\"].str.contains(\"High\")])\n",
    "        medium = len(df_schema_issues[df_schema_issues[\"severity\"].str.contains(\"Medium\")])\n",
    "        print(f\"   ğŸ”´ High:   {high}\")\n",
    "        print(f\"   ğŸŸ¡ Medium: {medium}\")\n",
    "        print()\n",
    "\n",
    "        for _, row in df_schema_issues.iterrows():\n",
    "            print(f\"   {row['severity']} [{row['item_name']}] {row['table']}.{row['column']}\")\n",
    "            print(f\"      Issue: {row['issue']}\")\n",
    "            print(f\"      Fix:   {row['recommendation']}\")\n",
    "            print()\n",
    "    else:\n",
    "        df_schema_issues = pd.DataFrame()\n",
    "        print(\"\\nâœ… No schema optimization issues detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f146c8",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Active Locks & Transaction Monitoring\n",
    "\n",
    "Checks for active locks and potential blocking using `sys.dm_tran_locks`. Long-running explicit transactions can cause contention with `SELECT` statements on system catalog views and the Fabric portal.\n",
    "\n",
    "> **Best Practice:** Keep transactions short-lived. Always `COMMIT` or `ROLLBACK`. Add retry logic with exponential backoff in pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Active Locks & Transaction Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "LOCKS_QUERY = \"\"\"(\n",
    "SELECT\n",
    "    l.resource_type,\n",
    "    l.request_mode,\n",
    "    l.request_status,\n",
    "    l.request_session_id     AS session_id,\n",
    "    CAST(t.name AS VARCHAR(128))   AS table_name,\n",
    "    COUNT(*)                 AS lock_count\n",
    "FROM sys.dm_tran_locks l\n",
    "LEFT JOIN sys.tables t ON l.resource_associated_entity_id = t.object_id\n",
    "    AND l.resource_type = 'OBJECT'\n",
    "GROUP BY l.resource_type, l.request_mode, l.request_status,\n",
    "         l.request_session_id, CAST(t.name AS VARCHAR(128))\n",
    "HAVING COUNT(*) > 0\n",
    ") AS lock_info\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_locks = []\n",
    "\n",
    "    for item in warehouses:\n",
    "        sql_ep = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ”’ Checking active locks for: {item_name} ...\")\n",
    "        df = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                        LOCKS_QUERY, \"Lock inspection\")\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"warehouse_name\"] = item_name\n",
    "            all_locks.append(df)\n",
    "\n",
    "            # Check for blocked requests (WAIT status)\n",
    "            blocked = df[df[\"request_status\"] == \"WAIT\"]\n",
    "            if not blocked.empty:\n",
    "                print(f\"   âš ï¸  {len(blocked)} blocked lock request(s) detected!\")\n",
    "            else:\n",
    "                print(f\"   âœ… Active locks present but no blocking ({len(df)} lock entries)\")\n",
    "        else:\n",
    "            print(f\"   âœ… No active locks\")\n",
    "\n",
    "    if all_locks:\n",
    "        df_locks = pd.concat(all_locks, ignore_index=True)\n",
    "        blocked_total = len(df_locks[df_locks[\"request_status\"] == \"WAIT\"])\n",
    "        if blocked_total > 0:\n",
    "            print(f\"\\nğŸ”’ LOCK CONTENTION ALERT\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"   Blocked requests: {blocked_total}\")\n",
    "            print(f\"\\nğŸ’¡ Tip: Investigate long-running transactions. Keep transactions short and use retry logic.\")\n",
    "    else:\n",
    "        df_locks = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828936a4",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Table Compaction & Row Group Health\n",
    "\n",
    "Checks for fragmented or suboptimal row groups that indicate tables needing compaction:\n",
    "- **Warehouse:** Data compaction runs automatically in the background\n",
    "- **Lakehouse:** Requires manual `OPTIMIZE` command via Spark or the Fabric portal\n",
    "\n",
    "> **Best Practice:** Avoid trickle inserts/updates/deletes. Batch operations to create efficient row groups (â‰¥100 MB files). For Lakehouse, run `OPTIMIZE` after significant data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Table Compaction & Row Group Health â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ROWGROUP_QUERY = \"\"\"(\n",
    "SELECT\n",
    "    s.name                                       AS schema_name,\n",
    "    t.name                                       AS table_name,\n",
    "    COUNT(DISTINCT ps.partition_number)           AS partition_count,\n",
    "    SUM(ps.row_count)                            AS total_rows,\n",
    "    COUNT(ps.partition_number)                   AS total_row_groups,\n",
    "    SUM(CASE\n",
    "        WHEN ps.row_count < 102400 THEN 1\n",
    "        ELSE 0\n",
    "    END)                                         AS small_row_groups,\n",
    "    AVG(ps.row_count)                            AS avg_rows_per_group,\n",
    "    MIN(ps.row_count)                            AS min_rows_in_group,\n",
    "    MAX(ps.row_count)                            AS max_rows_in_group\n",
    "FROM sys.dm_db_partition_stats ps\n",
    "INNER JOIN sys.tables t  ON ps.object_id = t.object_id\n",
    "INNER JOIN sys.schemas s ON t.schema_id = s.schema_id\n",
    "WHERE t.is_ms_shipped = 0\n",
    "    AND ps.index_id IN (0, 1)\n",
    "GROUP BY s.name, t.name\n",
    "HAVING SUM(ps.row_count) > 0\n",
    "ORDER BY SUM(CASE WHEN ps.row_count < 102400 THEN 1 ELSE 0 END) DESC\n",
    ") AS rowgroup_health\"\"\"\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    all_compaction = []\n",
    "\n",
    "    for item in warehouses + lakehouses:\n",
    "        sql_ep = item[\"sql_endpoint\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "        item_type = item[\"item_type\"]\n",
    "\n",
    "        if not sql_ep:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ—œï¸  Checking row group health for {item_type}: {item_name} ...\")\n",
    "        df = safe_query(sql_ep, item[\"database_name\"], sql_token,\n",
    "                        ROWGROUP_QUERY, \"Row group health\")\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"item_name\"] = item_name\n",
    "            df[\"item_type\"] = item_type\n",
    "            all_compaction.append(df)\n",
    "\n",
    "            # Flag tables with many small row groups\n",
    "            fragmented = df[df[\"small_row_groups\"] > 0]\n",
    "            if not fragmented.empty:\n",
    "                print(f\"   âš ï¸  {len(fragmented)} table(s) have small row groups (< 102,400 rows)\")\n",
    "                for _, row in fragmented.head(5).iterrows():\n",
    "                    pct = (row[\"small_row_groups\"] / max(row[\"total_row_groups\"], 1)) * 100\n",
    "                    print(f\"      - {row['schema_name']}.{row['table_name']}: \"\n",
    "                          f\"{row['small_row_groups']}/{row['total_row_groups']} small groups ({pct:.0f}%)\")\n",
    "            else:\n",
    "                print(f\"   âœ… All row groups are healthy\")\n",
    "\n",
    "    if all_compaction:\n",
    "        df_compaction = pd.concat(all_compaction, ignore_index=True)\n",
    "        print(f\"\\nğŸ—œï¸  TABLE COMPACTION HEALTH\")\n",
    "        print(\"=\" * 120)\n",
    "        total_tables = len(df_compaction)\n",
    "        fragmented_tables = len(df_compaction[df_compaction[\"small_row_groups\"] > 0])\n",
    "        print(f\"   Total tables analyzed:      {total_tables}\")\n",
    "        print(f\"   Tables with fragmentation:  {fragmented_tables}\")\n",
    "\n",
    "        if fragmented_tables > 0:\n",
    "            # Separate recommendations for Warehouse vs Lakehouse\n",
    "            wh_frag = df_compaction[(df_compaction[\"item_type\"] == \"Warehouse\") & (df_compaction[\"small_row_groups\"] > 0)]\n",
    "            lh_frag = df_compaction[(df_compaction[\"item_type\"] == \"Lakehouse\") & (df_compaction[\"small_row_groups\"] > 0)]\n",
    "            if not wh_frag.empty:\n",
    "                print(f\"\\n   ğŸ“¦ Warehouse ({len(wh_frag)} fragmented tables):\")\n",
    "                print(f\"      â„¹ï¸  Data compaction runs automatically. Consider batching writes to avoid trickle inserts.\")\n",
    "            if not lh_frag.empty:\n",
    "                print(f\"\\n   ğŸ  Lakehouse ({len(lh_frag)} fragmented tables):\")\n",
    "                print(f\"      ğŸ’¡ Run OPTIMIZE in Spark or via Fabric portal â†’ right-click table â†’ Maintenance â†’ Run now\")\n",
    "    else:\n",
    "        df_compaction = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc88f3",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ V-Order & Warehouse Configuration Check\n",
    "\n",
    "Checks V-Order status for each Warehouse. V-Order is a write-time optimization that improves read performance through better sorting and compression.\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| Read-heavy warehouse | Keep V-Order **ON** (default) |\n",
    "| Write-heavy staging warehouse | Consider disabling V-Order for ingestion throughput |\n",
    "| Staging â†’ Serving pattern | V-Order OFF on staging, ON on serving warehouse |\n",
    "\n",
    "> **Warning:** Once V-Order is disabled on a warehouse, it **cannot be re-enabled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ V-Order & Warehouse Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    vorder_results = []\n",
    "\n",
    "    for item in warehouses:\n",
    "        item_id   = item[\"item_id\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        print(f\"âš™ï¸  Checking V-Order for: {item_name} ...\")\n",
    "        try:\n",
    "            url = f\"{FABRIC_API_BASE}/workspaces/{WORKSPACE_ID}/warehouses/{item_id}\"\n",
    "            resp = requests.get(url, headers=HEADERS)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            props = data.get(\"properties\", {})\n",
    "\n",
    "            # V-Order is ON by default for warehouses\n",
    "            # The API may not expose it explicitly â€“ it's always ON unless disabled\n",
    "            vorder_status = \"ON (Default)\"\n",
    "            if \"vOrderEnabled\" in props:\n",
    "                vorder_status = \"ON\" if props[\"vOrderEnabled\"] else \"OFF âš ï¸\"\n",
    "\n",
    "            vorder_results.append({\n",
    "                \"item_name\": item_name,\n",
    "                \"item_type\": \"Warehouse\",\n",
    "                \"v_order\": vorder_status,\n",
    "                \"connection_string\": props.get(\"connectionString\", \"N/A\")[:80],\n",
    "            })\n",
    "            print(f\"   V-Order: {vorder_status}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Could not check: {str(e)[:80]}\")\n",
    "\n",
    "    # Also check Lakehouse maintenance settings\n",
    "    for item in lakehouses:\n",
    "        item_id   = item[\"item_id\"]\n",
    "        item_name = item[\"item_name\"]\n",
    "\n",
    "        print(f\"âš™ï¸  Checking maintenance for Lakehouse: {item_name} ...\")\n",
    "        try:\n",
    "            url = f\"{FABRIC_API_BASE}/workspaces/{WORKSPACE_ID}/lakehouses/{item_id}\"\n",
    "            resp = requests.get(url, headers=HEADERS)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            props = data.get(\"properties\", {})\n",
    "            maint = props.get(\"defaultTableMaintenanceConfiguration\", {})\n",
    "\n",
    "            vorder_status = \"OFF\"\n",
    "            compaction = \"OFF\"\n",
    "            if maint:\n",
    "                vorder_cfg = maint.get(\"vOrderOptimization\", {})\n",
    "                vorder_status = \"ON\" if vorder_cfg.get(\"enabled\", False) else \"OFF\"\n",
    "                compact_cfg = maint.get(\"fileCompaction\", {})\n",
    "                compaction = \"ON\" if compact_cfg.get(\"enabled\", False) else \"OFF\"\n",
    "\n",
    "            vorder_results.append({\n",
    "                \"item_name\": item_name,\n",
    "                \"item_type\": \"Lakehouse\",\n",
    "                \"v_order\": vorder_status,\n",
    "                \"compaction\": compaction,\n",
    "            })\n",
    "            print(f\"   V-Order: {vorder_status} | Compaction: {compaction}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Could not check: {str(e)[:80]}\")\n",
    "\n",
    "    if vorder_results:\n",
    "        df_vorder = pd.DataFrame(vorder_results)\n",
    "        print(f\"\\nâš™ï¸  V-ORDER & MAINTENANCE STATUS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(df_vorder.to_string(index=False))\n",
    "    else:\n",
    "        df_vorder = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaad2f9",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: OneLake Soft Delete Scanner\n",
    "\n",
    "Scans OneLake items for **soft-deleted files**. OneLake automatically retains deleted files for **7 days** before permanent removal. This section identifies soft-deleted files you can still recover.\n",
    "\n",
    "> **Key facts:**\n",
    "> - Soft-deleted files are retained for 7 days before permanent removal\n",
    "> - You pay for soft-deleted data at the same rate as active data\n",
    "> - Write access is required to restore files\n",
    "> - Use Azure Storage Explorer, PowerShell, or REST APIs to restore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb2aaf",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Scan OneLake for Soft-Deleted Files\n",
    "\n",
    "Uses the Azure Data Lake Storage SDK to enumerate soft-deleted files across all Lakehouses and Warehouses in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Scan OneLake for Soft-Deleted Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "\n",
    "class FabricStorageTokenCredential(TokenCredential):\n",
    "    \"\"\"Bridge Fabric notebook token to azure-storage credential.\"\"\"\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        tok = mssparkutils.credentials.getToken(\"storage\")\n",
    "        token_value = tok[\"accessToken\"] if isinstance(tok, dict) else tok\n",
    "        return AccessToken(token_value, int(time.time()) + 3600)\n",
    "\n",
    "\n",
    "def scan_soft_deleted_files(workspace_id, items, onelake_endpoint=\"https://onelake.dfs.fabric.microsoft.com\"):\n",
    "    \"\"\"\n",
    "    Scan OneLake items for soft-deleted files.\n",
    "    Returns a list of dicts with deleted file details.\n",
    "    \"\"\"\n",
    "    cred = FabricStorageTokenCredential()\n",
    "    service = DataLakeServiceClient(account_url=onelake_endpoint, credential=cred)\n",
    "    fs = service.get_file_system_client(workspace_id)\n",
    "\n",
    "    all_deleted = []\n",
    "    total_scanned = 0\n",
    "\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        item_id   = item[\"item_id\"]\n",
    "        item_name = item.get(\"item_name\", item_id)\n",
    "        item_type = item.get(\"item_type\", \"Unknown\")\n",
    "\n",
    "        print(f\"[{idx}/{len(items)}] Scanning {item_type}: {item_name} ...\")\n",
    "\n",
    "        try:\n",
    "            file_count = 0\n",
    "            deleted_count = 0\n",
    "\n",
    "            for path_props in fs.get_paths(path=item_id, recursive=True):\n",
    "                file_count += 1\n",
    "                total_scanned += 1\n",
    "\n",
    "                # Check if the file is soft-deleted\n",
    "                is_deleted = getattr(path_props, \"is_deleted\", False) or \\\n",
    "                             getattr(path_props, \"deleted\", False)\n",
    "\n",
    "                if is_deleted:\n",
    "                    deleted_count += 1\n",
    "                    name = getattr(path_props, \"name\", \"\") or \"\"\n",
    "                    size = int(getattr(path_props, \"content_length\", 0) or 0)\n",
    "                    deleted_time = getattr(path_props, \"deleted_time\", None) or \\\n",
    "                                   getattr(path_props, \"deletion_id\", None)\n",
    "                    remaining_days = getattr(path_props, \"remaining_retention_days\", None)\n",
    "\n",
    "                    # Get relative path\n",
    "                    rel_path = name[len(item_id):].lstrip(\"/\") if name.startswith(item_id) else name\n",
    "\n",
    "                    all_deleted.append({\n",
    "                        \"item_name\":      item_name,\n",
    "                        \"item_type\":      item_type,\n",
    "                        \"item_id\":        item_id,\n",
    "                        \"file_path\":      rel_path,\n",
    "                        \"size_bytes\":     size,\n",
    "                        \"size_mb\":        size / (1024 * 1024),\n",
    "                        \"deleted_time\":   str(deleted_time) if deleted_time else \"Unknown\",\n",
    "                        \"remaining_days\": remaining_days,\n",
    "                    })\n",
    "\n",
    "            if deleted_count > 0:\n",
    "                print(f\"   ğŸ—‘ï¸  Found {deleted_count} soft-deleted file(s) (out of {file_count} total)\")\n",
    "            else:\n",
    "                print(f\"   âœ… No soft-deleted files ({file_count} files scanned)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error scanning: {str(e)[:100]}\")\n",
    "\n",
    "    return all_deleted, total_scanned\n",
    "\n",
    "\n",
    "if RUN_SOFT_DELETE_SCAN:\n",
    "    # Filter items to requested types\n",
    "    scan_items = [item for item in all_items if item[\"item_type\"] in SCAN_ITEM_TYPES]\n",
    "\n",
    "    if scan_items:\n",
    "        print(f\"ğŸ—‘ï¸  Scanning {len(scan_items)} item(s) for soft-deleted files ...\\n\")\n",
    "        deleted_files, total_files_scanned = scan_soft_deleted_files(WORKSPACE_ID, scan_items)\n",
    "\n",
    "        df_deleted = pd.DataFrame(deleted_files) if deleted_files else pd.DataFrame()\n",
    "        print(f\"\\nğŸ“Š Soft Delete Scan Complete\")\n",
    "        print(f\"   Items scanned:        {len(scan_items)}\")\n",
    "        print(f\"   Files scanned:        {total_files_scanned:,}\")\n",
    "        print(f\"   Soft-deleted files:   {len(deleted_files)}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No items found matching the requested types for soft-delete scanning.\")\n",
    "        df_deleted = pd.DataFrame()\n",
    "else:\n",
    "    print(\"â­ï¸  Soft delete scan skipped (RUN_SOFT_DELETE_SCAN = False)\")\n",
    "    df_deleted = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf4baf",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Soft Delete Report & Recovery Guidance\n",
    "\n",
    "Detailed report of soft-deleted files with recovery guidance and storage cost impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed740021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Soft Delete Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def format_bytes(n):\n",
    "    \"\"\"Format bytes to human-readable size.\"\"\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    x = float(n)\n",
    "    i = 0\n",
    "    while x >= 1024 and i < len(units) - 1:\n",
    "        x /= 1024\n",
    "        i += 1\n",
    "    return f\"{x:,.2f} {units[i]}\" if i >= 2 else f\"{x:,.0f} {units[i]}\"\n",
    "\n",
    "\n",
    "if RUN_SOFT_DELETE_SCAN and not df_deleted.empty:\n",
    "    print(\"ğŸ—‘ï¸  ONELAKE SOFT DELETE REPORT\")\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"   Workspace:          {ws['displayName']}\")\n",
    "    print(f\"   Total deleted files: {len(df_deleted)}\")\n",
    "\n",
    "    total_deleted_bytes = df_deleted[\"size_bytes\"].sum()\n",
    "    print(f\"   Total deleted size:  {format_bytes(total_deleted_bytes)}\")\n",
    "    print(f\"   âš ï¸  You are billed for soft-deleted data at the same rate as active data\")\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    # â”€â”€ Summary by item â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ“Š Soft-Deleted Files by Item\")\n",
    "    print(\"-\" * 80)\n",
    "    item_summary = df_deleted.groupby([\"item_type\", \"item_name\"]).agg(\n",
    "        file_count=(\"file_path\", \"count\"),\n",
    "        total_size_bytes=(\"size_bytes\", \"sum\"),\n",
    "    ).reset_index()\n",
    "    item_summary[\"total_size\"] = item_summary[\"total_size_bytes\"].apply(format_bytes)\n",
    "\n",
    "    for _, row in item_summary.iterrows():\n",
    "        icon = \"ğŸ“¦\" if row[\"item_type\"] == \"Warehouse\" else \"ğŸ \"\n",
    "        print(f\"   {icon} {row['item_name']}: {row['file_count']} files ({row['total_size']})\")\n",
    "\n",
    "    # â”€â”€ File extension breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ“ Deleted Files by Type\")\n",
    "    print(\"-\" * 60)\n",
    "    df_deleted[\"file_ext\"] = df_deleted[\"file_path\"].apply(\n",
    "        lambda x: x.rsplit(\".\", 1)[-1].lower() if \".\" in str(x) else \"unknown\"\n",
    "    )\n",
    "    ext_summary = df_deleted.groupby(\"file_ext\").agg(\n",
    "        count=(\"file_path\", \"count\"),\n",
    "        size_bytes=(\"size_bytes\", \"sum\")\n",
    "    ).sort_values(\"size_bytes\", ascending=False)\n",
    "\n",
    "    for ext, row in ext_summary.iterrows():\n",
    "        print(f\"   .{ext:12s}  {row['count']:6,} files  {format_bytes(row['size_bytes']):>12s}\")\n",
    "\n",
    "    # â”€â”€ Urgency â€“ files expiring soon â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if \"remaining_days\" in df_deleted.columns:\n",
    "        expiring = df_deleted[df_deleted[\"remaining_days\"].notna()].copy()\n",
    "        if not expiring.empty:\n",
    "            expiring[\"remaining_days\"] = pd.to_numeric(expiring[\"remaining_days\"], errors=\"coerce\")\n",
    "            urgent = expiring[expiring[\"remaining_days\"] <= 2]\n",
    "            if not urgent.empty:\n",
    "                print(f\"\\nğŸš¨ URGENT: {len(urgent)} file(s) expiring within 2 days!\")\n",
    "                print(\"-\" * 80)\n",
    "                for _, row in urgent.head(10).iterrows():\n",
    "                    print(f\"   â° {row['item_name']}/{row['file_path']} \"\n",
    "                          f\"({format_bytes(row['size_bytes'])}) â€“ {row['remaining_days']:.0f} day(s) left\")\n",
    "\n",
    "    # â”€â”€ Recovery instructions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"\\n\\nğŸ’¡ HOW TO RECOVER SOFT-DELETED FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\"\"\n",
    "   Option 1: Azure Storage Explorer\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   1. Open Azure Storage Explorer â†’ Connect to OneLake\n",
    "   2. Navigate to the item containing deleted files\n",
    "   3. Change dropdown to \"Active and soft deleted blobs\"\n",
    "   4. Right-click the deleted file â†’ Undelete\n",
    "\n",
    "   Option 2: PowerShell\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   $ctx = New-AzStorageContext -StorageAccountName \"onelake\" \\\\\n",
    "          -UseConnectedAccount -endpoint \"fabric.microsoft.com\"\n",
    "   Get-AzStorageBlob -Container \"<workspace-name>\" -Context $ctx \\\\\n",
    "          -Prefix \"<item>.Lakehouse/Files/\" -IncludeDeleted |\n",
    "       Where-Object { $_.IsDeleted } | Restore-AzStorageBlob\n",
    "\n",
    "   Option 3: REST API\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   PUT https://onelake.blob.fabric.microsoft.com/<workspace>/<path>?comp=undelete\n",
    "    \"\"\")\n",
    "\n",
    "    # â”€â”€ Detailed file listing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"\\nğŸ“‹ Detailed Soft-Deleted Files (showing first 50)\")\n",
    "    print(\"-\" * 120)\n",
    "    display_cols = [\"item_type\", \"item_name\", \"file_path\", \"size_mb\", \"deleted_time\", \"remaining_days\"]\n",
    "    avail_cols = [c for c in display_cols if c in df_deleted.columns]\n",
    "    print(df_deleted[avail_cols].head(50).to_string(index=False))\n",
    "\n",
    "elif RUN_SOFT_DELETE_SCAN:\n",
    "    print(\"âœ… No soft-deleted files found across all scanned items.\")\n",
    "    print(\"   OneLake soft delete retains deleted files for 7 days before permanent deletion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399d8a0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£3ï¸âƒ£ Consolidated Performance Summary & Recommendations\n",
    "\n",
    "Combined performance health dashboard with prioritized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bf057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Consolidated Performance Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"ğŸ“‹ CONSOLIDATED PERFORMANCE & DATA PROTECTION SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"   Workspace: {ws['displayName']}\")\n",
    "print(f\"   Warehouses: {len(warehouses)}  |  Lakehouses: {len(lakehouses)}\")\n",
    "print(f\"   Report Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "recommendations = []\n",
    "priority_order = {\"ğŸ”´ Critical\": 0, \"ğŸŸ  High\": 1, \"ğŸŸ¡ Medium\": 2, \"ğŸŸ¢ Info\": 3}\n",
    "\n",
    "# â”€â”€ Part A Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    print(\"\\nğŸš€ PART A: WAREHOUSE PERFORMANCE\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Cold cache\n",
    "    if 'df_cold_cache' in dir() and not df_cold_cache.empty:\n",
    "        print(f\"   â„ï¸  Cold Cache:     {len(df_cold_cache)} queries with significant remote storage reads\")\n",
    "        recommendations.append({\n",
    "            \"priority\": \"ğŸŸ¡ Medium\",\n",
    "            \"area\": \"Cold Cache\",\n",
    "            \"recommendation\": f\"{len(df_cold_cache)} queries fetched data from remote storage. \"\n",
    "                             \"Don't judge performance on first execution. Consider warming cache for critical queries.\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"   â„ï¸  Cold Cache:     âœ… No issues\")\n",
    "\n",
    "    # Long-running queries\n",
    "    if 'df_long_running' in dir() and not df_long_running.empty:\n",
    "        print(f\"   ğŸ¢ Long-Running:   {len(df_long_running)} slow query patterns\")\n",
    "        recommendations.append({\n",
    "            \"priority\": \"ğŸŸ  High\",\n",
    "            \"area\": \"Query Performance\",\n",
    "            \"recommendation\": f\"{len(df_long_running)} long-running query patterns found. \"\n",
    "                             \"Review query plans, add data clustering, and optimize JOINs/filters.\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"   ğŸ¢ Long-Running:   âœ… No issues\")\n",
    "\n",
    "    # Statistics\n",
    "    if 'df_all_stats' in dir() and not df_all_stats.empty:\n",
    "        stale = len(df_all_stats[df_all_stats[\"modification_counter\"] > 0])\n",
    "        print(f\"   ğŸ“Š Statistics:     {stale} with modifications since last update\")\n",
    "        if stale > 0:\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸŸ¡ Medium\",\n",
    "                \"area\": \"Statistics\",\n",
    "                \"recommendation\": f\"{stale} statistics have pending modifications. \"\n",
    "                                 \"Run UPDATE STATISTICS during maintenance windows.\"\n",
    "            })\n",
    "    else:\n",
    "        print(f\"   ğŸ“Š Statistics:     âœ… All fresh\")\n",
    "\n",
    "    # Schema issues\n",
    "    if 'df_schema_issues' in dir() and not df_schema_issues.empty:\n",
    "        print(f\"   ğŸ” Schema Issues:  {len(df_schema_issues)} optimization opportunities\")\n",
    "        high_issues = len(df_schema_issues[df_schema_issues[\"severity\"].str.contains(\"High\")])\n",
    "        if high_issues > 0:\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸŸ  High\",\n",
    "                \"area\": \"Data Types\",\n",
    "                \"recommendation\": f\"{high_issues} columns with oversized string types (varchar(8000)/max). \"\n",
    "                                 \"Use specific varchar(n) lengths for better statistics and query plans.\"\n",
    "            })\n",
    "    else:\n",
    "        print(f\"   ğŸ” Schema Issues:  âœ… Well-optimized\")\n",
    "\n",
    "    # Compaction\n",
    "    if 'df_compaction' in dir() and not df_compaction.empty:\n",
    "        fragmented = len(df_compaction[df_compaction[\"small_row_groups\"] > 0])\n",
    "        print(f\"   ğŸ—œï¸  Compaction:     {fragmented} tables with fragmentation\")\n",
    "        if fragmented > 0:\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸŸ¡ Medium\",\n",
    "                \"area\": \"Compaction\",\n",
    "                \"recommendation\": f\"{fragmented} tables have small row groups. \"\n",
    "                                 \"For Lakehouse: run OPTIMIZE. For Warehouse: batch writes to avoid trickle inserts.\"\n",
    "            })\n",
    "    else:\n",
    "        print(f\"   ğŸ—œï¸  Compaction:     âœ… Healthy\")\n",
    "\n",
    "    # Locks\n",
    "    if 'df_locks' in dir() and not df_locks.empty:\n",
    "        blocked = len(df_locks[df_locks[\"request_status\"] == \"WAIT\"])\n",
    "        if blocked > 0:\n",
    "            print(f\"   ğŸ”’ Locks:          âš ï¸  {blocked} blocked request(s)\")\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸ”´ Critical\",\n",
    "                \"area\": \"Locking\",\n",
    "                \"recommendation\": f\"{blocked} blocked lock requests detected. \"\n",
    "                                 \"Investigate long-running transactions. Keep transactions short-lived.\"\n",
    "            })\n",
    "        else:\n",
    "            print(f\"   ğŸ”’ Locks:          âœ… No blocking\")\n",
    "    else:\n",
    "        print(f\"   ğŸ”’ Locks:          âœ… No active locks\")\n",
    "\n",
    "# â”€â”€ Part B Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if RUN_SOFT_DELETE_SCAN:\n",
    "    print(f\"\\nğŸ—‘ï¸  PART B: ONELAKE SOFT DELETE\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    if not df_deleted.empty:\n",
    "        total_deleted_size = df_deleted[\"size_bytes\"].sum()\n",
    "        print(f\"   Soft-deleted files:  {len(df_deleted)}\")\n",
    "        print(f\"   Deleted data size:   {format_bytes(total_deleted_size)}\")\n",
    "        print(f\"   ğŸ’° Storage cost:     Same as active data â€“ review if unneeded\")\n",
    "\n",
    "        if total_deleted_size > 1073741824:  # > 1 GB\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸŸ  High\",\n",
    "                \"area\": \"Soft Delete\",\n",
    "                \"recommendation\": f\"{format_bytes(total_deleted_size)} of soft-deleted data. \"\n",
    "                                 \"You are billed at the same rate as active data. Review if this data needs recovery.\"\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                \"priority\": \"ğŸŸ¢ Info\",\n",
    "                \"area\": \"Soft Delete\",\n",
    "                \"recommendation\": f\"{len(df_deleted)} soft-deleted files ({format_bytes(total_deleted_size)}). \"\n",
    "                                 \"Files auto-expire after 7 days. Recover using Storage Explorer or PowerShell if needed.\"\n",
    "            })\n",
    "    else:\n",
    "        print(f\"   âœ… No soft-deleted files found\")\n",
    "\n",
    "# â”€â”€ Prioritized Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n\\nğŸ’¡ PRIORITIZED RECOMMENDATIONS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if recommendations:\n",
    "    sorted_recs = sorted(recommendations, key=lambda x: priority_order.get(x[\"priority\"], 99))\n",
    "    for i, rec in enumerate(sorted_recs, 1):\n",
    "        print(f\"\\n   {i}. {rec['priority']} [{rec['area']}]\")\n",
    "        print(f\"      {rec['recommendation']}\")\n",
    "else:\n",
    "    print(\"   âœ… All checks passed! Your workspace is performing well.\")\n",
    "\n",
    "print(f\"\\n\\nğŸ“š REFERENCE LINKS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   ğŸ“– Performance Guidelines:  https://learn.microsoft.com/en-us/fabric/data-warehouse/guidelines-warehouse-performance\")\n",
    "print(\"   ğŸ“– OneLake Soft Delete:      https://learn.microsoft.com/en-us/fabric/onelake/onelake-disaster-recovery#soft-delete-for-onelake-files\")\n",
    "print(\"   ğŸ“– Recover Deleted Files:    https://learn.microsoft.com/en-us/fabric/onelake/soft-delete\")\n",
    "print(\"   ğŸ“– Query Insights:           https://learn.microsoft.com/en-us/fabric/data-warehouse/query-insights\")\n",
    "print(\"   ğŸ“– Data Clustering:          https://learn.microsoft.com/en-us/fabric/data-warehouse/data-clustering\")\n",
    "print(\"   ğŸ“– Table Maintenance:        https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25341649",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£4ï¸âƒ£ Export Reports to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export Reports to CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "exports = []\n",
    "\n",
    "# â”€â”€ Performance Exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if RUN_PERFORMANCE_DIAGNOSTICS:\n",
    "    if 'df_cold_cache' in dir() and not df_cold_cache.empty:\n",
    "        fname = f\"perf_cold_cache_{timestamp}.csv\"\n",
    "        df_cold_cache.to_csv(fname, index=False)\n",
    "        exports.append((\"Cold Cache Queries\", fname, len(df_cold_cache)))\n",
    "\n",
    "    if 'df_long_running' in dir() and not df_long_running.empty:\n",
    "        fname = f\"perf_long_running_queries_{timestamp}.csv\"\n",
    "        df_long_running.to_csv(fname, index=False)\n",
    "        exports.append((\"Long-Running Queries\", fname, len(df_long_running)))\n",
    "\n",
    "    if 'df_all_stats' in dir() and not df_all_stats.empty:\n",
    "        fname = f\"perf_statistics_freshness_{timestamp}.csv\"\n",
    "        df_all_stats.to_csv(fname, index=False)\n",
    "        exports.append((\"Statistics Freshness\", fname, len(df_all_stats)))\n",
    "\n",
    "    if 'df_schema_issues' in dir() and not df_schema_issues.empty:\n",
    "        fname = f\"perf_schema_issues_{timestamp}.csv\"\n",
    "        df_schema_issues.to_csv(fname, index=False)\n",
    "        exports.append((\"Schema Issues\", fname, len(df_schema_issues)))\n",
    "\n",
    "    if 'df_compaction' in dir() and not df_compaction.empty:\n",
    "        fname = f\"perf_compaction_health_{timestamp}.csv\"\n",
    "        df_compaction.to_csv(fname, index=False)\n",
    "        exports.append((\"Compaction Health\", fname, len(df_compaction)))\n",
    "\n",
    "# â”€â”€ Soft Delete Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if RUN_SOFT_DELETE_SCAN and not df_deleted.empty:\n",
    "    fname = f\"soft_deleted_files_{timestamp}.csv\"\n",
    "    df_deleted.to_csv(fname, index=False)\n",
    "    exports.append((\"Soft-Deleted Files\", fname, len(df_deleted)))\n",
    "\n",
    "# â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if exports:\n",
    "    print(\"âœ… Reports exported:\")\n",
    "    for name, fname, rows in exports:\n",
    "        print(f\"   ğŸ“„ {name}: {fname} ({rows} rows)\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa41bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Notes\n",
    "\n",
    "### Performance Checks Reference\n",
    "\n",
    "| Check | DMV / API | What It Detects |\n",
    "|-------|-----------|-----------------|\n",
    "| Cold Cache | `queryinsights.exec_requests_history` | Queries loading data from OneLake instead of cache |\n",
    "| Long-Running | `queryinsights.long_running_queries` | Queries with high median/max execution time |\n",
    "| Frequently Run | `queryinsights.frequently_run_queries` | Most-executed queries (optimization candidates) |\n",
    "| Statistics | `sys.stats` + `dm_db_stats_properties` | Stale statistics needing update |\n",
    "| Schema | `sys.columns` + `sys.types` | Oversized strings, inefficient data types |\n",
    "| Locks | `sys.dm_tran_locks` | Active locks and blocking |\n",
    "| Compaction | `sys.dm_db_partition_stats` | Small/fragmented row groups |\n",
    "| V-Order | Fabric REST API | V-Order and maintenance configuration |\n",
    "\n",
    "### OneLake Soft Delete Key Facts\n",
    "\n",
    "| Fact | Detail |\n",
    "|------|--------|\n",
    "| Retention Period | 7 days (fixed, not configurable) |\n",
    "| Billing | Soft-deleted data billed at same rate as active data |\n",
    "| Recovery Methods | Azure Storage Explorer, PowerShell (`Az.Storage`), REST API |\n",
    "| Access Required | Write access to the item |\n",
    "| Scope | All OneLake files (Lakehouse, Warehouse) |\n",
    "\n",
    "### Best Practices Summary (from Microsoft Guidelines)\n",
    "\n",
    "1. **Statistics**: Run `UPDATE STATISTICS` during maintenance windows after bulk data changes\n",
    "2. **String columns**: Use specific `varchar(n)` lengths, avoid `varchar(8000)` or `varchar(max)`\n",
    "3. **Data types**: Use `int`/`bigint` instead of `decimal(18,0)` for whole numbers\n",
    "4. **Transactions**: Keep short-lived, always COMMIT/ROLLBACK, add retry logic\n",
    "5. **Ingestion**: Batch writes, use `COPY INTO` with 100 MBâ€“1 GB files, avoid trickle inserts\n",
    "6. **Compaction**: Warehouse auto-compacts; for Lakehouse run `OPTIMIZE` after significant changes\n",
    "7. **V-Order**: Keep enabled for read-heavy workloads; only disable for write-heavy staging\n",
    "8. **Data clustering**: Cluster columns used in WHERE/JOIN predicates for file skipping\n",
    "9. **Nullable columns**: Use `NOT NULL` when the data model allows\n",
    "10. **Region alignment**: Keep data in the same region as your Fabric capacity\n",
    "\n",
    "### Permissions Required\n",
    "- **Workspace Admin** or **Member** role for full access\n",
    "- **Write access** to items for restoring soft-deleted files"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
