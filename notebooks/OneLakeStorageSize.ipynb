{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9140f-25b3-4296-adc9-bccab8998209",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# %pip -q install azure-storage-file-datalake\n",
    "\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "from collections import defaultdict\n",
    "from pathlib import PurePosixPath\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fabric token -> TokenCredential (for OneLake DFS)\n",
    "# -----------------------------\n",
    "class FabricStorageTokenCredential(TokenCredential):\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        import notebookutils\n",
    "        tok = notebookutils.credentials.getToken(\"storage\")\n",
    "        token_value = tok[\"accessToken\"] if isinstance(tok, dict) else tok\n",
    "        return AccessToken(token_value, int(time.time()) + 3600)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fabric REST helpers\n",
    "# -----------------------------\n",
    "def _get_fabric_rest_token():\n",
    "    import notebookutils\n",
    "    return notebookutils.credentials.getToken(\"pbi\")\n",
    "\n",
    "\n",
    "def _fabric_get(url: str, token: str):\n",
    "    r = requests.get(url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    if r.status_code >= 400:\n",
    "        raise RuntimeError(f\"Fabric REST call failed {r.status_code}: {r.text}\")\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "\n",
    "def list_lakehouses_and_warehouses(workspace_id: str):\n",
    "    \"\"\"\n",
    "    Returns list of dicts: {id, displayName, type}\n",
    "    \"\"\"\n",
    "    token = _get_fabric_rest_token()\n",
    "    # Items endpoint includes Lakehouse/Warehouse among others\n",
    "    data = _fabric_get(f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\", token)\n",
    "    items = data.get(\"value\", [])\n",
    "\n",
    "    wanted = []\n",
    "    for it in items:\n",
    "        t = it.get(\"type\")\n",
    "        if t in (\"Lakehouse\", \"Warehouse\"):\n",
    "            wanted.append({\n",
    "                \"id\": it.get(\"id\"),\n",
    "                \"displayName\": it.get(\"displayName\"),\n",
    "                \"type\": t\n",
    "            })\n",
    "    return wanted\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregation helpers\n",
    "# -----------------------------\n",
    "def parent_dirs(path: PurePosixPath, max_depth=None):\n",
    "    \"\"\"\n",
    "    Yield parent directories for a file path, from deepest to root.\n",
    "    Root is represented as \"\" internally.\n",
    "    max_depth: limit rollup depth (\"a\" depth=1, \"a/b\" depth=2, etc.)\n",
    "    \"\"\"\n",
    "    parts = path.parts\n",
    "    dir_parts = parts[:-1]\n",
    "    if not dir_parts:\n",
    "        yield \"\"  # root\n",
    "        return\n",
    "\n",
    "    for d in range(len(dir_parts), 0, -1):\n",
    "        if max_depth is not None and d > max_depth:\n",
    "            continue\n",
    "        yield \"/\".join(dir_parts[:d])\n",
    "\n",
    "    yield \"\"  # root\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main: enumerate items, then list each itemId in OneLake\n",
    "# -----------------------------\n",
    "def measure_onelake_workspace_by_items(\n",
    "    workspace_name: str | None = None,\n",
    "    item_types: tuple[str, ...] = (\"Lakehouse\", \"Warehouse\"),\n",
    "    onelake_dfs_endpoint: str = \"https://onelake.dfs.fabric.microsoft.com\",\n",
    "    max_depth: int | None = None,\n",
    "    progress_every_files: int = 200_000,\n",
    "    prefix_with_item: bool = True,   # prefix directory keys with \"<Type>/<Name>/...\"\n",
    "):\n",
    "    import sempy \n",
    "    \"\"\"\n",
    "    Enumerate Lakehouses/Warehouses in a workspace and compute:\n",
    "      - total bytes\n",
    "      - per-item total bytes\n",
    "      - per-directory cumulative bytes\n",
    "\n",
    "    Uses OneLake GUID addressing: list each item by fs.get_paths(path=itemId, recursive=True).\n",
    "    \"\"\"\n",
    "\n",
    "    if workspace_name == None:\n",
    "        workspace_name =  notebookutils.runtime.context.get(\"currentWorkspaceName\")\n",
    "\n",
    "    workspace_id = sempy.fabric.resolve_workspace_id(workspace_name)\n",
    "    items = list_lakehouses_and_warehouses(workspace_id)\n",
    "    items = [it for it in items if it[\"type\"] in item_types]\n",
    "\n",
    "    if not items:\n",
    "        raise RuntimeError(f\"No items of type {item_types} found in workspace ({workspace_id}).\")\n",
    "\n",
    "    cred = FabricStorageTokenCredential()\n",
    "    service = DataLakeServiceClient(account_url=onelake_dfs_endpoint, credential=cred)\n",
    "    fs = service.get_file_system_client(workspace_id)\n",
    "\n",
    "    dir_bytes = defaultdict(int)\n",
    "    item_totals = []\n",
    "    grand_total = 0\n",
    "    grand_files = 0\n",
    "    started = time.time()\n",
    "\n",
    "    for idx, it in enumerate(items, start=1):\n",
    "        item_id = it[\"id\"]\n",
    "        item_name = it.get(\"displayName\") or item_id\n",
    "        item_type = it[\"type\"]\n",
    "\n",
    "        item_bytes = 0\n",
    "        item_files = 0\n",
    "\n",
    "        item_prefix = f\"{item_type}/{item_name}\".replace(\"\\\\\", \"/\").strip(\"/\")\n",
    "        list_path = item_id  # GUID root for this item\n",
    "\n",
    "        print(f\"[{idx}/{len(items)}] Listing {item_type}: {item_name} ({item_id})\")\n",
    "\n",
    "        for p in fs.get_paths(path=list_path, recursive=True):\n",
    "            if getattr(p, \"is_directory\", False):\n",
    "                continue\n",
    "\n",
    "            size = int(getattr(p, \"content_length\", 0) or 0)\n",
    "            item_bytes += size\n",
    "            item_files += 1\n",
    "            grand_total += size\n",
    "            grand_files += 1\n",
    "\n",
    "            # p.name begins with \"<itemId>/...\"\n",
    "            name = p.name or \"\"\n",
    "            rel = name[len(item_id):].lstrip(\"/\") if name.startswith(item_id) else name\n",
    "            key_path = f\"{item_prefix}/{rel}\" if prefix_with_item else rel\n",
    "\n",
    "            pp = PurePosixPath(key_path)\n",
    "            for d in parent_dirs(pp, max_depth=max_depth):\n",
    "                dir_bytes[d] += size\n",
    "\n",
    "            if progress_every_files and grand_files % progress_every_files == 0:\n",
    "                elapsed = time.time() - started\n",
    "                rate = grand_files / elapsed if elapsed > 0 else 0\n",
    "                print(f\"  Processed {grand_files:,} files total | {grand_total/1e12:.3f} TB | {rate:,.0f} files/sec\")\n",
    "\n",
    "        item_totals.append({\n",
    "            \"type\": item_type,\n",
    "            \"item_name\": item_name,\n",
    "            \"item_id\": item_id,\n",
    "            \"files\": item_files,\n",
    "            \"bytes\": item_bytes,\n",
    "        })\n",
    "\n",
    "    elapsed = time.time() - started\n",
    "\n",
    "    # Directory dataframe\n",
    "    rows = [(d if d != \"\" else \"/\", b) for d, b in dir_bytes.items()]\n",
    "    df_dirs = pd.DataFrame(rows, columns=[\"directory\", \"bytes\"])\n",
    "    df_dirs[\"gb\"] = df_dirs[\"bytes\"] / (1024**3)\n",
    "    df_dirs[\"tb\"] = df_dirs[\"bytes\"] / (1024**4)\n",
    "    df_dirs = df_dirs.sort_values(\"bytes\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Item totals dataframe\n",
    "    df_items = pd.DataFrame(item_totals)\n",
    "    df_items[\"gb\"] = df_items[\"bytes\"] / (1024**3)\n",
    "    df_items[\"tb\"] = df_items[\"bytes\"] / (1024**4)\n",
    "    df_items = df_items.sort_values(\"bytes\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    summary = {\n",
    "        \"workspace_id\": workspace_id,\n",
    "        \"workspace_name\": workspace_name,\n",
    "        \"items_count\": len(items),\n",
    "        \"files\": grand_files,\n",
    "        \"total_bytes\": grand_total,\n",
    "        \"total_gb\": grand_total / (1024**3),\n",
    "        \"total_tb\": grand_total / (1024**4),\n",
    "        \"seconds\": elapsed,\n",
    "        \"files_per_sec\": (grand_files / elapsed) if elapsed > 0 else None,\n",
    "        \"dirs_tracked\": len(dir_bytes),\n",
    "        \"max_depth\": max_depth,\n",
    "        \"prefix_with_item\": prefix_with_item,\n",
    "        \"onelake_dfs_endpoint\": onelake_dfs_endpoint,\n",
    "    }\n",
    "\n",
    "    return summary, df_dirs, df_items\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run (defaults to current workspace)\n",
    "# -----------------------------\n",
    "summary, df_dirs, df_items = measure_onelake_workspace_by_items(\n",
    "    workspace_name=None,     # or \"My Workspace Name\"\n",
    "    max_depth=4,             # set None for full rollup (can be huge)\n",
    "    prefix_with_item=True,\n",
    "    progress_every_files=100_000\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21686fcf-9950-4bb9-8ea3-506a7234dadd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def format_bytes(n: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\",\"EB\"]\n",
    "    x = float(n)\n",
    "    i = 0\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024\n",
    "        i += 1\n",
    "    return f\"{x:,.2f} {units[i]}\" if i >= 3 else f\"{x:,.0f} {units[i]}\"\n",
    "\n",
    "def print_top_folders(df_dirs, top_n=10):\n",
    "    top = df_dirs.sort_values(\"bytes\", ascending=False).head(top_n)\n",
    "\n",
    "    print(f\"\\nTop {len(top)} folders by size\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in top.iterrows():\n",
    "        print(f\"{row['directory']:<60} {format_bytes(int(row['bytes'])):>15}\")\n",
    "\n",
    "# Run it\n",
    "print_top_folders(df_dirs, top_n=50)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": null
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
